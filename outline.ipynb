{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Stock Forecaster  \n",
    "**(FMP + Kronos + FinText-TSFM | Signal-Only, Point-in-Time Safe)**\n",
    "\n",
    "---\n",
    "\n",
    "## 0) Project Explanation & Philosophy\n",
    "\n",
    "### What this project is\n",
    "\n",
    "This project builds a **decision-support forecasting model** that answers one core question:\n",
    "\n",
    "> **Which AI stocks are most attractive to buy today, on a risk-adjusted basis, over the next 20 / 60 / 90 trading days?**\n",
    "\n",
    "The system outputs **ranked stock recommendations and return distributions**, not trades.  \n",
    "Its purpose is to generate **credible alpha signals** that survive realistic financial constraints.\n",
    "\n",
    "The design explicitly accounts for:\n",
    "- non-stationary market behavior,\n",
    "- weak and noisy financial signals,\n",
    "- transaction costs and liquidity effects,\n",
    "- and strict point-in-time (PIT) correctness.\n",
    "\n",
    "---\n",
    "\n",
    "### What this project is NOT\n",
    "\n",
    "This project does **not**:\n",
    "- place trades,\n",
    "- connect to brokers,\n",
    "- optimize execution,\n",
    "- or manage live capital.\n",
    "\n",
    "Any portfolio-related logic exists **only to validate signal realism**, not to implement trading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Core modeling philosophy\n",
    "\n",
    "1. **Ranking beats regression**  \n",
    "   Relative ordering of stocks is more stable and economically useful than exact price prediction.\n",
    "\n",
    "2. **Point-in-time correctness is non-negotiable**  \n",
    "   Any signal unavailable at time *T* must not influence predictions at time *T*.\n",
    "\n",
    "3. **Economic validity > statistical fit**  \n",
    "   Signals must survive transaction costs, turnover, and regime shifts.\n",
    "\n",
    "4. **Multiple weak signals > single strong model**  \n",
    "   Combine complementary views:\n",
    "   - price dynamics (Kronos),\n",
    "   - return structure (FinText-TSFM),\n",
    "   - fundamentals and context (tabular models)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) System Outputs (Signal-Only)\n",
    "\n",
    "At each rebalance date **T**, for each stock and horizon (20 / 60 / 90 trading days):\n",
    "\n",
    "### Per-stock outputs\n",
    "- **Expected excess return** vs benchmark (QQQ default; XLK/SMH optional)\n",
    "- **Return distribution** (5th / 50th / 95th percentiles)\n",
    "- **Alpha ranking score** (cross-sectional)\n",
    "- **Confidence score** (calibrated uncertainty)\n",
    "- **Key drivers** (feature blocks influencing the rank)\n",
    "\n",
    "### Cross-sectional outputs\n",
    "- Ranked list: **Top buys / neutral / avoid**\n",
    "- Optional confidence buckets (high vs low confidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Scope & Validation Philosophy (Signal-Only)\n",
    "\n",
    "### Scope\n",
    "- The system produces **signals**, not trades.\n",
    "- No execution or order placement logic is implemented.\n",
    "\n",
    "### Why portfolio concepts still appear\n",
    "Portfolio concepts (turnover, costs, constraints) are used **only for evaluation realism**, to answer:\n",
    "> *Would these signals remain economically meaningful if followed by an investor?*\n",
    "\n",
    "### Optional realism check\n",
    "- Paper trading (e.g., Alpaca paper) may be used **post-hoc** to validate:\n",
    "  - timestamp integrity,\n",
    "  - universe construction,\n",
    "  - signal stability.\n",
    "- Paper trading results are **never** used for training or model selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Data & Point-in-Time Infrastructure (FMP-First)\n",
    "\n",
    "### 3.1 Data sources\n",
    "- **Market**: Daily OHLCV, splits, dividends\n",
    "- **Fundamentals**: Income, balance sheet, cash flow (quarterly)\n",
    "- **Metadata**: Sector, industry, shares outstanding, market cap\n",
    "- **Events**: Earnings dates with announcement time\n",
    "- **Benchmarks**: QQQ (default), optional XLK / SMH\n",
    "- **Regime proxies**: VIX, market breadth, rate proxies\n",
    "\n",
    "---\n",
    "\n",
    "### 3.2 Point-in-time (PIT) rules\n",
    "\n",
    "Each datapoint stores:\n",
    "- `value`\n",
    "- `observed_at` (first public release timestamp)\n",
    "- `effective_from`\n",
    "- `source`\n",
    "\n",
    "Rules:\n",
    "- Fundamentals are **as-reported**, never restated historically\n",
    "- Forward-fill allowed **only after** `observed_at`\n",
    "- No feature may use information released after the cutoff time\n",
    "\n",
    "---\n",
    "\n",
    "### 3.3 Daily cutoff policy (anti-lookahead)\n",
    "\n",
    "- Fixed cutoff time (e.g., 4:00pm ET)\n",
    "- Features for date *T* may only use data with timestamps \u2264 cutoff(T)\n",
    "- Earnings handling distinguishes pre-market vs after-close announcements\n",
    "\n",
    "---\n",
    "\n",
    "### 3.4 Data audits & bias detection\n",
    "\n",
    "Automated checks:\n",
    "- PIT violation scanner\n",
    "- Survivorship reconstruction audit\n",
    "- Corporate action sanity checks\n",
    "- Missingness and outlier detection\n",
    "\n",
    "**Success criteria**\n",
    "- < 0.1% PIT violations\n",
    "- Universe reproducible for any historical date\n",
    "- All datasets auditable and replayable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Survivorship-Safe Dynamic Universe\n",
    "\n",
    "### 4.1 Universe construction (critical)\n",
    "\n",
    "At each rebalance date **T**:\n",
    "- Start with all U.S. equities meeting liquidity and price thresholds\n",
    "- Filter by AI-relevant sector / industry tags\n",
    "- Select **Top N by market cap as-of T**\n",
    "- Persist constituents with timestamp\n",
    "\n",
    "Hardcoded \"today's winners\" are explicitly disallowed.\n",
    "\n",
    "---\n",
    "\n",
    "### 4.2 Delistings & mergers\n",
    "- Delisted stocks remain in historical universes where data exists\n",
    "- Missing data is explicitly modeled rather than silently dropped\n",
    "\n",
    "**Success criteria**\n",
    "- Constituents vary meaningfully through time\n",
    "- Backtests include both winners and failures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Feature Engineering (Bias-Safe)\n",
    "\n",
    "### 5.0 Readiness Checklist & Implementation Plan\n",
    "\n",
    "#### Infrastructure Available (from Chapters 3-4) \u2705\n",
    "| Component | Module | What It Provides |\n",
    "|-----------|--------|------------------|\n",
    "| Prices | `FMPClient.get_historical_prices()` | Split-adjusted OHLCV with `observed_at` |\n",
    "| Fundamentals | `FMPClient.get_income_statement()` etc. | With `fillingDate` for PIT |\n",
    "| Volume/ADV | `DuckDBPITStore.get_avg_volume()` | Computed from OHLCV |\n",
    "| Events | `EventStore` | EARNINGS, FILING, SENTIMENT with PIT |\n",
    "| Earnings | `AlphaVantageClient` + `ExpectationsClient` | BMO/AMC timing, surprises |\n",
    "| Regime/VIX | `FMPClient.get_index_historical()` | SPY, VIX for regime detection |\n",
    "| Universe | `UniverseBuilder` | FULL survivorship via Polygon |\n",
    "| ID Mapping | `SecurityMaster` | Stable IDs, ticker changes |\n",
    "| Calendar | `TradingCalendarImpl` | NYSE holidays, cutoffs |\n",
    "| Caching | All clients | `data/cache/*` directories |\n",
    "\n",
    "#### API Keys Available \u2705\n",
    "- `FMP_KEYS` - Prices, fundamentals, profiles (free tier: 250/day)\n",
    "- `POLYGON_KEYS` - Symbol master, universe (free tier: 5/min)\n",
    "- `ALPHAVANTAGE_KEYS` - Earnings calendar (free tier: 25/day)\n",
    "\n",
    "---\n",
    "\n",
    "#### Chapter 5 TODO List \u2705 COMPLETE\n",
    "\n",
    "**5.1 Targets (Labels)** \u2705\n",
    "- [x] Implement forward excess return calculation vs QQQ benchmark\n",
    "- [x] Create label generator for 20/60/90 trading day horizons\n",
    "- [x] Ensure labels are strictly PIT-safe (no future leakage)\n",
    "- [x] **v2: Total return labels (dividends included) - DEFAULT**\n",
    "\n",
    "**5.2 Price & Volume Features** \u2705\n",
    "- [x] Momentum features (1m, 3m, 6m, 12m returns)\n",
    "- [x] Volatility (realized vol, vol-of-vol)\n",
    "- [x] Drawdown (max drawdown, current vs high)\n",
    "- [x] Relative strength vs universe median\n",
    "- [x] Beta vs benchmark (rolling window)\n",
    "- [x] ADV and volatility-adjusted ADV\n",
    "\n",
    "**5.3 Fundamental Features (Relative)** \u2705\n",
    "- [x] P/E vs own 3-year history (z-score)\n",
    "- [x] P/S vs sector median\n",
    "- [x] Margins vs sector peers\n",
    "- [x] Revenue/earnings growth vs sector\n",
    "- [x] All ratios rank-transformed cross-sectionally\n",
    "\n",
    "**5.4 Event & Calendar Features** \u2705\n",
    "- [x] Days to next earnings\n",
    "- [x] Days since last earnings\n",
    "- [x] Post-earnings drift window indicator (PEAD 63 days)\n",
    "- [x] Surprise magnitude (last N quarters)\n",
    "- [x] Surprise streak and cross-sectional z-score\n",
    "- [x] Filing recency (days since last 10-Q/10-K)\n",
    "\n",
    "**5.5 Regime & Macro Features** \u2705\n",
    "- [x] VIX level and percentile (2-year window)\n",
    "- [x] VIX regime classification (low/normal/elevated/high)\n",
    "- [x] Market trend regime (bull/bear/neutral)\n",
    "- [x] Sector rotation indicators (tech vs defensives)\n",
    "- [x] All features timestamped with cutoff enforcement\n",
    "\n",
    "**5.6 Missingness Masks** \u2705\n",
    "- [x] Create explicit \"known at time T\" indicators\n",
    "- [x] Missingness as first-class feature (not just imputation)\n",
    "- [x] Track data coverage statistics by category\n",
    "- [x] Generate coverage reports\n",
    "\n",
    "**5.7 Feature Hygiene & Redundancy** \u2705\n",
    "- [x] Cross-sectional z-score/rank standardization\n",
    "- [x] Rolling Spearman correlation matrix\n",
    "- [x] Feature clustering (identify blocks)\n",
    "- [x] VIF diagnostics (tabular features)\n",
    "- [x] Rolling IC stability checks\n",
    "- [x] Sign consistency analysis\n",
    "\n",
    "**5.8 Feature Neutralization (Diagnostics)** \u2705\n",
    "- [x] Sector-neutral IC computation\n",
    "- [x] Beta-neutral IC computation\n",
    "- [x] Sector+Beta neutral IC computation\n",
    "- [x] Delta (\u0394) reporting for interpretation\n",
    "\n",
    "**Testing & Validation** \u2705\n",
    "- [x] Unit tests for each feature block (5.1-5.7 all have tests)\n",
    "- [x] PIT violation scanner on all features\n",
    "- [x] Univariate IC \u2265 0.03 check for strong signals (IC tools available)\n",
    "- [x] IC stability across rolling windows (FeatureHygiene.compute_ic_stability)\n",
    "- [x] Feature coverage > 95% (MissingnessTracker.compute_coverage_stats)\n",
    "\n",
    "---\n",
    "\n",
    "### 5.1 Targets (Labels) \u2705 COMPLETE\n",
    "**Implemented in `src/features/labels.py`**\n",
    "\n",
    "**v2 (DEFAULT): Total Return Labels**\n",
    "- Forward excess returns include dividends\n",
    "- Formula: `TR_i,T(H) - TR_b,T(H)` where TR = price return + dividend yield\n",
    "- Horizons: 20 / 60 / 90 trading days\n",
    "- PIT-safe with `label_matured_at` timestamps\n",
    "\n",
    "**v1 (Legacy): Price-Only Labels**\n",
    "- Available via `label_version='v1'` flag\n",
    "- For backward compatibility only\n",
    "\n",
    "---\n",
    "\n",
    "### 5.2 Price & volume features \u2705 COMPLETE\n",
    "**Implemented in `src/features/price_features.py`**\n",
    "\n",
    "| Feature | Description |\n",
    "|---------|-------------|\n",
    "| `mom_1m/3m/6m/12m` | Returns over 21/63/126/252 trading days |\n",
    "| `vol_20d/60d` | Annualized volatility |\n",
    "| `vol_of_vol` | Volatility of rolling volatility |\n",
    "| `max_drawdown_60d` | Maximum drawdown |\n",
    "| `rel_strength_1m/3m` | Z-score vs universe |\n",
    "| `beta_252d` | Beta vs QQQ benchmark |\n",
    "| `adv_20d/60d` | Average daily dollar volume |\n",
    "\n",
    "---\n",
    "\n",
    "### 5.3 Fundamentals (relative, normalized) \u2705 COMPLETE\n",
    "**Implemented in `src/features/fundamental_features.py`**\n",
    "\n",
    "Raw ratios are avoided \u2014 all features are RELATIVE:\n",
    "- `pe_zscore_3y`: P/E vs own 3-year history\n",
    "- `pe_vs_sector`: P/E relative to sector median\n",
    "- `ps_vs_sector`: P/S relative to sector median\n",
    "- `gross_margin_vs_sector`: Margins vs sector\n",
    "- `revenue_growth_vs_sector`: Growth vs sector peers\n",
    "- `roe_zscore`, `roa_zscore`: Quality metrics z-scored\n",
    "\n",
    "---\n",
    "\n",
    "### Time-Decay Sample Weighting (Training Policy) \u2705\n",
    "**Implemented in `src/features/time_decay.py`**\n",
    "\n",
    "**Why time decay matters for AI stocks:**\n",
    "- AI business models and the \"AI regime\" (2020+) differ from earlier eras\n",
    "- Market microstructure evolves (HFT, retail flow)\n",
    "- Many AI stocks didn't exist 15+ years ago \u2014 that's OK\n",
    "- Recent observations are more relevant for forward predictions\n",
    "\n",
    "**Recommended half-lives:**\n",
    "| Horizon | Half-Life | Weight at 6y | Weight at 9y |\n",
    "|---------|-----------|--------------|--------------|\n",
    "| 20d     | 2.5 years | ~18%         | ~7%          |\n",
    "| 60d     | 3.5 years | ~30%         | ~14%         |\n",
    "| 90d     | 4.5 years | ~38%         | ~21%         |\n",
    "\n",
    "---\n",
    "\n",
    "### 5.4 Events & calendars \u2705 COMPLETE\n",
    "**Implemented in `src/features/event_features.py`**\n",
    "\n",
    "| Feature | Description |\n",
    "|---------|-------------|\n",
    "| `days_to_earnings` | Days until next expected earnings |\n",
    "| `days_since_earnings` | Days since last report |\n",
    "| `in_pead_window` | Post-earnings drift window (63 days) |\n",
    "| `last_surprise_pct` | Most recent surprise % |\n",
    "| `avg_surprise_4q` | Rolling 4Q average |\n",
    "| `surprise_streak` | Consecutive beats/misses |\n",
    "| `surprise_zscore` | Cross-sectional z-score |\n",
    "| `days_since_10k/10q` | Filing recency |\n",
    "| `reports_bmo` | Typical announcement timing |\n",
    "\n",
    "---\n",
    "\n",
    "### 5.5 Regime & macro \u2705 COMPLETE\n",
    "**Implemented in `src/features/regime_features.py`**\n",
    "\n",
    "| Feature | Description |\n",
    "|---------|-------------|\n",
    "| `vix_level`, `vix_percentile` | VIX level and 2-year percentile |\n",
    "| `vix_regime` | low/normal/elevated/high |\n",
    "| `market_return_5d/21d/63d` | SPY returns at various windows |\n",
    "| `market_regime` | bull/bear/neutral (MA-based) |\n",
    "| `above_ma_50`, `above_ma_200` | Price vs moving averages |\n",
    "| `tech_vs_staples/utilities` | Sector rotation signals |\n",
    "\n",
    "---\n",
    "\n",
    "### 5.6 Availability & missingness masks \u2705 COMPLETE\n",
    "**Implemented in `src/features/missingness.py`**\n",
    "\n",
    "| Feature | Description |\n",
    "|---------|-------------|\n",
    "| `coverage_pct` | Overall feature coverage (0-1) |\n",
    "| `{category}_coverage` | Per-category availability |\n",
    "| `has_{type}_data` | Boolean availability flags |\n",
    "| `is_new_stock` | < 1 year of history |\n",
    "\n",
    "**Key Philosophy:** Missingness is a SIGNAL, not just noise.\n",
    "\n",
    "---\n",
    "\n",
    "### 5.7 Feature Hygiene & Redundancy Control \u2705 COMPLETE\n",
    "**Implemented in `src/features/hygiene.py`**\n",
    "\n",
    "| Component | Description |\n",
    "|-----------|-------------|\n",
    "| Cross-sectional standardization | z-score or rank-transform within date |\n",
    "| Rolling Spearman correlation | Correlation matrix computation |\n",
    "| Feature clustering | Hierarchical clustering to identify blocks |\n",
    "| VIF diagnostics | Variance Inflation Factor (diagnostic, not filter) |\n",
    "| IC stability analysis | Rolling IC with sign consistency tracking |\n",
    "\n",
    "> **Principle**: A feature with IC 0.04 once and \u22120.01 later is worse than IC 0.02 stable forever.\n",
    "\n",
    "---\n",
    "\n",
    "### 5.8 Feature Neutralization \u2705 COMPLETE\n",
    "**Implemented in `src/features/neutralization.py`**\n",
    "\n",
    "**Purpose:** For diagnostics ONLY (not training). Reveals WHERE alpha comes from.\n",
    "\n",
    "| Component | Description |\n",
    "|-----------|-------------|\n",
    "| Sector-neutral IC | IC after removing sector effects |\n",
    "| Beta-neutral IC | IC after removing market beta |\n",
    "| Sector+Beta neutral IC | IC after removing both factors |\n",
    "| Delta (\u0394) reporting | neutral_IC - raw_IC for interpretation |\n",
    "\n",
    "**Interpretation:**\n",
    "- Large negative \u0394_sector \u2192 feature was mostly sector rotation\n",
    "- Large negative \u0394_beta \u2192 feature was mostly market exposure\n",
    "- Small \u0394 \u2192 alpha is genuinely stock-specific\n",
    "\n",
    "---\n",
    "\n",
    "**Feature success criteria** \u2705\n",
    "- > 95% completeness (post-masking)\n",
    "- Strong univariate signals show IC \u2273 0.03\n",
    "- No feature introduces PIT violations\n",
    "- **Stability**: IC sign consistent across \u226570% of rolling windows\n",
    "- **Redundancy understood**: Feature blocks documented, correlation matrix computed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Evaluation Framework (Core Credibility Layer) \u2705 CLOSED & FROZEN\n",
    "\n",
    "---\n",
    "\n",
    "### \ud83d\udd12 CHAPTER 6 FREEZE STATUS\n",
    "\n",
    "**Status:** CLOSED & FROZEN (December 30, 2025)  \n",
    "**Tests:** 413/413 passing  \n",
    "**Commits:**\n",
    "- `18bad8a` - Chapter 6: Closure fixes + freeze REAL baseline reference\n",
    "- `7e6fa3a` - Chapter 6: Freeze REAL baseline reference artifacts\n",
    "\n",
    "**Frozen Baseline Floor (REAL DuckDB Data):**\n",
    "\n",
    "| Horizon | Best Baseline | Median RankIC | Quintile Spread | Hit Rate @10 | N Folds |\n",
    "|---------|---------------|---------------|-----------------|--------------|---------|\n",
    "| **20d** | `mom_12m_monthly` | **0.0283** | 0.0035 | 0.50 | 109 |\n",
    "| **60d** | `momentum_composite_monthly` | **0.0392** | 0.0370 | 0.60 | 109 |\n",
    "| **90d** | `momentum_composite_monthly` | **0.0169** | 0.0374 | 0.60 | 109 |\n",
    "\n",
    "**Sanity Check:** \u2705 PASSED (`naive_random` RankIC \u2248 0 for all horizons)\n",
    "\n",
    "**Frozen Artifacts (tracked in git):**\n",
    "- `evaluation_outputs/chapter6_closure_real/` - Baseline reference (IMMUTABLE)\n",
    "- `BASELINE_FLOOR.json` - Metrics to beat for Chapter 7+\n",
    "- `BASELINE_REFERENCE.md` - Usage instructions\n",
    "- `CLOSURE_MANIFEST.json` - Commit hash (`bf2cf8e`), data hash (`5723d4c88b8ecba1...`)\n",
    "\n",
    "**Data Snapshot:**\n",
    "- Source: DuckDB (`data/features.duckdb`)\n",
    "- Rows: 192,307 (2016-01-04 \u2192 2025-02-19)\n",
    "- Tickers: ~100 (AI universe)\n",
    "- Horizons: 20d, 60d, 90d (TRADING DAYS)\n",
    "- Label Version: v2 (total return with dividends)\n",
    "\n",
    "**Reference Doc:** See `CHAPTER_6_FREEZE.md` for complete details.\n",
    "\n",
    "**What This Means:**\n",
    "- \u2705 Chapter 6 evaluation pipeline is COMPLETE and may not be modified\n",
    "- \u2705 Baseline reference is FROZEN and is the immutable comparison anchor for Chapter 7+\n",
    "- \u2705 All future models must use this frozen pipeline and beat the frozen baseline floor\n",
    "- \u26a0\ufe0f Any changes to evaluation definitions require a new version and complete re-freeze\n",
    "\n",
    "---\n",
    "\n",
    "> **CRITICAL PHILOSOPHY**: \"You've crossed the line where bad evaluation can ruin a good system.\"\n",
    ">\n",
    "> **Approach**: Be conservative. Let results look \"boring\" if they are. Resist the urge to tweak features/models early. If signals survive Chapter 6 as-is, Chapters 7-11 will feel almost easy.\n",
    "\n",
    "### 6.0 Prerequisites Check \u2705\n",
    "- **Labels**: v2 total return (dividends), mature-aware, PIT-safe \u2705\n",
    "- **Features**: 5.1-5.8 complete, stable, interpretable, auditable \u2705\n",
    "- **Missingness**: Explicit, not dropped \u2705\n",
    "- **Regime**: Visible but not leaked \u2705\n",
    "- **Alpha attribution**: Neutralization working \u2705\n",
    "- **PIT discipline**: Scanner enforced, 0 CRITICAL violations \u2705\n",
    "\n",
    "---\n",
    "\n",
    "### 6.0.2 Definition Lock \u2705 IMPLEMENTED\n",
    "\n",
    "> **CRITICAL**: All time conventions are locked in `src/evaluation/definitions.py`\n",
    "\n",
    "**Canonical Definitions (FROZEN):**\n",
    "\n",
    "| Parameter | Value | Unit | Enforcement |\n",
    "|-----------|-------|------|-------------|\n",
    "| **Horizons** | 20, 60, 90 | TRADING DAYS | `validate_horizon()` |\n",
    "| **Embargo** | 90 | TRADING DAYS | `validate_embargo()` |\n",
    "| **Rebalance** | 1st of month | Calendar day | Walk-forward splitter |\n",
    "| **Pricing** | Close-to-close | - | Label generator |\n",
    "| **Maturity** | label_matured_at <= cutoff_utc | UTC datetime | fold.filter_labels() |\n",
    "\n",
    "**Anti-Leakage Enforcement (HARD CONSTRAINTS):**\n",
    "```python\n",
    "# Embargo validation (raises ValueError if < 90 TRADING DAYS)\n",
    "from src.evaluation import validate_embargo\n",
    "validate_embargo(90)  # \u2705 Passes\n",
    "validate_embargo(30)  # \u274c ValueError: \"must be at least 90 TRADING DAYS\"\n",
    "\n",
    "# Maturity check (UTC datetime, not naive date)\n",
    "from src.evaluation import get_market_close_utc, is_label_mature\n",
    "cutoff_utc = get_market_close_utc(date(2023, 6, 15))  # 4 PM ET \u2192 UTC\n",
    "is_label_mature(label_matured_at, cutoff_date)  # Rejects naive datetimes\n",
    "```\n",
    "\n",
    "**Purging Rules (Per-Row-Per-Horizon):**\n",
    "- Train labels: Purge if T + H (trading days) > train_end\n",
    "- Val labels: Purge if T - H (trading days) < train_end\n",
    "- NOT a global rule that \"happens to work because embargo = 90\"\n",
    "\n",
    "**End-of-Sample Eligibility:**\n",
    "- All horizons (20/60/90) must be valid for an as-of date\n",
    "- No partial horizons near end of evaluation period\n",
    "- `require_all_horizons=True` in splitter\n",
    "\n",
    "**Test Coverage:** 65/65 tests passing\n",
    "- `tests/test_definitions.py`: 40 tests\n",
    "- `tests/test_walk_forward.py`: 25 tests\n",
    "\n",
    "---\n",
    "\n",
    "### 6.0.1 Pre-Implementation Sanity Checks (COMPLETE BEFORE CODING)\n",
    "\n",
    "> **CRITICAL**: These are not blockers\u2014they are sanity locks. Complete both before writing any Chapter 6 code.\n",
    "\n",
    "#### \u2705 Sanity Check 1: Manual IC vs Qlib IC Parity Test\n",
    "\n",
    "**Purpose:** Ensure adapter/indexing is correct before generating hundreds of evaluation reports.\n",
    "\n",
    "**Test Protocol:**\n",
    "```python\n",
    "# One fold, one horizon, same predictions\n",
    "fold = \"2023-Q1\"\n",
    "horizon = 20\n",
    "\n",
    "# Manual RankIC calculation\n",
    "manual_rankic = df.groupby(\"date\").apply(\n",
    "    lambda x: spearmanr(x[\"prediction\"], x[\"label\"])[0]\n",
    ").median()\n",
    "\n",
    "# Qlib RankIC calculation\n",
    "qlib_df = adapter.to_qlib_format(predictions, labels)\n",
    "qlib_rankic = qlib.evaluate(qlib_df)[\"IC\"].median()\n",
    "\n",
    "# STOP if they don't match\n",
    "assert abs(manual_rankic - qlib_rankic) < 0.001, \"IC mismatch - fix adapter!\"\n",
    "```\n",
    "\n",
    "**Acceptance:** Manual and Qlib RankIC must agree to 3 decimal places.\n",
    "\n",
    "**If they don't match \u2192 STOP immediately:**\n",
    "- Check MultiIndex formatting (datetime, instrument)\n",
    "- Check date alignment (T vs T+H)\n",
    "- Check for missing data handling differences\n",
    "- Check for sign flips (prediction vs label)\n",
    "\n",
    "#### \u2705 Sanity Check 2: Experiment Naming Convention\n",
    "\n",
    "**Purpose:** Prevent chaos when Recorder usage explodes across hundreds of experiments.\n",
    "\n",
    "**Convention (LOCK THIS IN NOW):**\n",
    "```\n",
    "exp = ai_forecaster/\n",
    "      horizon={20,60,90}/\n",
    "      model={kronos_v0, fintext_v0, tabular_lgb, baseline_mom12m}/\n",
    "      labels={v1_priceonly, v2_totalreturn}/\n",
    "      fold={01, 02, ..., 40}/\n",
    "```\n",
    "\n",
    "**Example paths:**\n",
    "```\n",
    "ai_forecaster/horizon=20/model=kronos_v0/labels=v2/fold=03\n",
    "ai_forecaster/horizon=60/model=baseline_mom12m/labels=v2/fold=12\n",
    "ai_forecaster/horizon=90/model=tabular_lgb/labels=v2/fold=25\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 6.1 Walk-Forward Engine\n",
    "\n",
    "**Expanding window** (not rolling):\n",
    "- Grows forward, never shrinks\n",
    "- Preserves long-term signal stability\n",
    "- Avoids artificial lookback dependency\n",
    "\n",
    "**Rebalance Cadence (LOCKED):**\n",
    "\n",
    "| Frequency | Purpose | Details |\n",
    "|-----------|---------|---------|\n",
    "| **Monthly (Primary)** | Main evaluation | First trading day of month, ~110 points (2016-2025) |\n",
    "| **Quarterly (Secondary)** | Robustness check | Supplementary slice only |\n",
    "\n",
    "**Evaluation Date Range (LOCKED):**\n",
    "```python\n",
    "EVAL_START = \"2016-01-01\"  # Earliest reliable fundamentals + universe snapshots\n",
    "EVAL_END = \"2025-06-30\"    # Conservative: guarantees 90d label maturity\n",
    "```\n",
    "\n",
    "**Why these dates?**\n",
    "- **2016-01-01**: FMP fundamentals reliable, universe coverage sufficient\n",
    "- **2025-06-30**: All 90d labels mature (PIT-safe), includes 2023-25 AI rally\n",
    "- **Result**: ~110 monthly points, multiple regimes (pre-COVID, COVID, drawdown, AI mania)\n",
    "\n",
    "**Universe snapshots:**\n",
    "- Uses `stable_id` snapshots from Chapter 4 (survivorship-safe)\n",
    "- Respects `label_matured_at` timestamps (PIT-safe)\n",
    "- No forward-looking information leakage\n",
    "\n",
    "**Time-decay sample weighting** (training only):\n",
    "- From `src/features/time_decay.py`\n",
    "- Horizon-specific half-lives: 2.5y (20d), 3.5y (60d), 4.5y (90d)\n",
    "- Per-date normalization for cross-sectional ranking\n",
    "\n",
    "---\n",
    "\n",
    "### 6.1.1 Baselines (Models to Beat)\n",
    "\n",
    "**3 baselines to beat:**\n",
    "\n",
    "| Baseline | Feature(s) | Purpose |\n",
    "|----------|-----------|---------|\n",
    "| **A: `mom_12m`** | 12-month momentum | Primary naive baseline (embarrassing if we can't beat) |\n",
    "| **B: `momentum_composite`** | `(mom_1m + mom_3m + mom_6m + mom_12m) / 4` | Stronger but transparent |\n",
    "| **C: `short_term_strength`** | `mom_1m` or `rel_strength_1m` | Diagnostic for horizon sensitivity |\n",
    "\n",
    "**+ 1 sanity baseline (not a target):**\n",
    "- **`naive_random`**: Deterministic random scores (sanity check: RankIC \u2248 0, confirms no systematic bias)\n",
    "\n",
    "**Critical Guardrail:**\n",
    "All baselines run through **identical pipeline:**\n",
    "- Same universe snapshots (`stable_id`)\n",
    "- Same missingness handling\n",
    "- Same neutralization setting (raw or sector/beta-neutral)\n",
    "- Same cost diagnostic treatment (6.4)\n",
    "- Same purging/embargo\n",
    "- Same walk-forward splits\n",
    "\n",
    "**No baseline shopping:** Adding more baselines = temptation to cherry-pick weak ones.\n",
    "\n",
    "---\n",
    "\n",
    "### 6.2 Label Hygiene\n",
    "**Enforce maturity rule:**\n",
    "- `label_matured_at <= asof` strictly enforced\n",
    "- No label is used before it matures\n",
    "\n",
    "**Horizon-aware purging:**\n",
    "- Remove overlapping labels across training/validation folds\n",
    "- Prevents leakage from correlated label windows\n",
    "\n",
    "**Embargo = max horizon:**\n",
    "- Gap between train and validation = 90 trading days\n",
    "- Conservative cushion for all horizons (20/60/90)\n",
    "\n",
    "---\n",
    "\n",
    "### 6.3 Metrics (Ranking-First)\n",
    "\n",
    "**Primary Metric: RankIC**\n",
    "- Spearman correlation of predicted ranks vs actual excess returns\n",
    "- More stable than Pearson IC (robust to outliers)\n",
    "- Directly measures ranking quality\n",
    "\n",
    "**IC by Regime:**\n",
    "- VIX low/high quartiles\n",
    "- Bull/bear markets (SPY 200-day MA)\n",
    "- Sector rotation periods\n",
    "\n",
    "**Top-Bottom Quintile Spread:**\n",
    "- Return of top 20% - return of bottom 20%\n",
    "- Measures practical exploitability\n",
    "\n",
    "**Top-K Definition (LOCKED):**\n",
    "\n",
    "| Metric | Primary | Secondary | Target |\n",
    "|--------|---------|-----------|--------|\n",
    "| **Top-K size** | Top-10 | Top-20 | - |\n",
    "| **Churn** | Jaccard or % retained | - | < 30% |\n",
    "| **Hit Rate** | Excess return > 0 | - | > 55% |\n",
    "\n",
    "**Churn formula:**\n",
    "```python\n",
    "churn = 1 - len(set(top_k_t) & set(top_k_t_minus_1)) / len(set(top_k_t) | set(top_k_t_minus_1))\n",
    "```\n",
    "\n",
    "**Hit Rate definition:**\n",
    "```python\n",
    "hit = (top_k_portfolio_return - benchmark_return) > 0\n",
    "hit_rate = hits / total_rebalances\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 6.4 Cost Realism (Diagnostic)\n",
    "\n",
    "**Base costs:**\n",
    "- 20 bps round-trip (conservative for liquid largecaps)\n",
    "- Higher for small/mid caps if needed\n",
    "\n",
    "**ADV-scaled slippage:**\n",
    "- Function of position size / average daily dollar volume\n",
    "- Penalizes illiquid names\n",
    "\n",
    "**Question: \"Does alpha survive?\"**\n",
    "- This is diagnostic, NOT optimization\n",
    "- If alpha vanishes post-cost \u2192 reject signal\n",
    "- If survives \u2192 document slippage sensitivity\n",
    "\n",
    "---\n",
    "\n",
    "### 6.5 Stability Reports\n",
    "\n",
    "**IC decay plots:**\n",
    "- How does IC degrade over time within a fold?\n",
    "- Stable signals show flat or slow decay\n",
    "- Rapid decay \u2192 overfitting or regime shift\n",
    "\n",
    "**Regime-conditional performance:**\n",
    "- IC in VIX high vs low\n",
    "- IC in bull vs bear\n",
    "- IC across sectors\n",
    "\n",
    "**Churn diagnostics:**\n",
    "- Top-10 ranking turnover month-over-month\n",
    "- High churn (>50%) \u2192 unstable, costly\n",
    "- Target: <30% for exploitability\n",
    "\n",
    "---\n",
    "\n",
    "### 6.6 Qlib Integration (Shadow Evaluator)\n",
    "\n",
    "**Philosophy:** Use Microsoft's Qlib as a \"shadow evaluator\" for standardized reporting, NOT as a replacement for our core infrastructure.\n",
    "\n",
    "**What Qlib Does for Us:**\n",
    "\n",
    "| Chapter 6 Component | Qlib Feature |\n",
    "|---------------------|--------------|\n",
    "| **6.3 Metrics** | Built-in IC/RankIC analysis, monthly IC, regime IC, autocorrelation plots |\n",
    "| **6.5 Reporting** | Quintile analysis, cumulative returns, long-short distribution, drawdown |\n",
    "| **6.4 Cost Realism** | Backtest engine with configurable transaction costs (second opinion) |\n",
    "| **Experiment Tracking** | Recorder system for managing walk-forward folds and model variants |\n",
    "\n",
    "**Integration Pattern (Narrow & Safe):**\n",
    "```\n",
    "Our System (source of truth) \u2192 predictions + labels \u2192 Qlib \u2192 evaluation reports\n",
    "```\n",
    "\n",
    "**Data Flow:**\n",
    "1. Our pipeline generates: `(date, ticker, prediction, label, optional_group)`\n",
    "2. Qlib receives this DataFrame (not raw data/features)\n",
    "3. Qlib outputs: standardized factor evaluation + backtest summaries\n",
    "\n",
    "**What Qlib Does NOT Replace:**\n",
    "- \u274c Universe construction (we keep stable_id + survivorship)\n",
    "- \u274c Feature engineering (we keep PIT discipline + 5.1-5.8)\n",
    "- \u274c Label generation (we keep v2 total return)\n",
    "- \u274c Data storage (we keep DuckDB PIT store)\n",
    "\n",
    "**Implementation:**\n",
    "```python\n",
    "# Adapter layer\n",
    "def our_predictions_to_qlib_format(predictions_df, labels_df):\n",
    "    qlib_df = pd.merge(predictions_df, labels_df, on=[\"date\", \"ticker\"])\n",
    "    qlib_df = qlib_df.set_index([\"datetime\", \"instrument\"])  # Qlib format\n",
    "    return qlib_df\n",
    "\n",
    "# Generate reports\n",
    "from qlib.contrib.evaluate import backtest_daily\n",
    "reports = backtest_daily(prediction=qlib_df, ...)\n",
    "```\n",
    "\n",
    "**References:**\n",
    "- Qlib GitHub: https://github.com/microsoft/qlib\n",
    "- Qlib Docs: https://qlib.readthedocs.io/en/latest/\n",
    "- Evaluation: https://qlib.readthedocs.io/en/latest/component/report.html\n",
    "\n",
    "---\n",
    "\n",
    "**Acceptance Criteria:**\n",
    "- \u2705 Median walk-forward RankIC > baseline by \u2265 0.02\n",
    "- \u2705 Net-of-cost improvement: % positive folds \u2265 baseline + 10pp (relative gate; frozen floor: 5.8%-40.1%)\n",
    "- \u2705 Top-10 ranking churn < 30% month-over-month\n",
    "- \u2705 Performance degrades gracefully under regime shifts\n",
    "- \u2705 NO PIT violations (enforced by scanner)\n",
    "\n",
    "**Guardrails:**\n",
    "- \u274c NO new features mid-evaluation\n",
    "- \u274c NO retraining models to \"fix\" bad folds\n",
    "- \u274c NO cherry-picking good time periods\n",
    "- \u274c NO optimizing to costs (diagnostic only)\n",
    "- \u274c NO hiding negative results\n",
    "\n",
    "**Success = Boring Results That Don't Break**\n",
    "- Median IC of 0.03-0.05 is GOOD\n",
    "- Stable across regimes is EXCELLENT\n",
    "- Survives costs is SUFFICIENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Baseline Models (Models to Beat) \u2705 IMPLEMENTED\n",
    "\n",
    "---\n",
    "\n",
    "### 7.0 Baseline Philosophy\n",
    "\n",
    "Baselines establish the **floor** that models must clear. They serve three purposes:\n",
    "\n",
    "1. **Sanity check**: If a model can't beat momentum, something is fundamentally wrong\n",
    "2. **Value demonstration**: ML must add measurable value over transparent alternatives  \n",
    "3. **Stability anchor**: Frozen baselines prevent \"drifting targets\" during model iteration\n",
    "\n",
    "**Critical rule**: Baselines are **locked before any model is trained**. No baseline shopping.\n",
    "\n",
    "---\n",
    "\n",
    "### 7.1 Baseline Categories\n",
    "\n",
    "All baselines run through the **identical evaluation pipeline** as models:\n",
    "- Same universe snapshots (stable_id + survivorship)\n",
    "- Same walk-forward folds (purging/embargo/maturity)\n",
    "- Same EvaluationRow contract\n",
    "- Same metrics, costs, and stability reports\n",
    "\n",
    "#### 7.1.1 Factor Baselines (Models Must Beat)\n",
    "\n",
    "| Baseline | Description | Formula | Purpose |\n",
    "|----------|-------------|---------|---------|\n",
    "| `mom_12m` | 12-month momentum | `score = mom_12m` | **Primary naive baseline** \u2014 If models can't beat this, something is wrong |\n",
    "| `momentum_composite` | Multi-horizon momentum | `score = (mom_1m + mom_3m + mom_6m + mom_12m) / 4` | **Stronger transparent baseline** \u2014 Realistic bar for \"is ML worth it?\" |\n",
    "| `short_term_strength` | 1-month momentum | `score = mom_1m` | **Diagnostic baseline** \u2014 Exposes horizon sensitivity and mean-reversion regimes |\n",
    "\n",
    "#### 7.1.2 Sanity Baselines (Pipeline Verification Only)\n",
    "\n",
    "| Baseline | Description | Formula | Purpose |\n",
    "|----------|-------------|---------|---------|\n",
    "| `naive_random` | Deterministic random | `score = hash(as_of_date, horizon, stable_id)` | **Pipeline sanity** \u2014 If RankIC \u2260 ~0, evaluation is hallucinating alpha |\n",
    "\n",
    "**Note**: `naive_random` should NEVER be used as a \"bar to clear\". It's purely a sanity check.\n",
    "\n",
    "#### 7.1.3 ML Baselines (Tuned, Then Frozen)\n",
    "\n",
    "| Baseline | Description | Status |\n",
    "|----------|-------------|--------|\n",
    "| `tabular_lgb` | LightGBM on feature stack | \ud83d\udd04 TODO (Chapter 7) |\n",
    "\n",
    "The ML baseline establishes: \"Does deep learning add value over tuned gradient boosting?\"\n",
    "\n",
    "---\n",
    "\n",
    "### 7.2 Baseline Gates (Pass/Fail Thresholds)\n",
    "\n",
    "These are the minimum thresholds for each baseline category:\n",
    "\n",
    "| Gate | Metric | Threshold | What It Means |\n",
    "|------|--------|-----------|---------------|\n",
    "| **Factor Gate** | median RankIC (best factor) | \u2265 0.02 | Momentum signal exists in data |\n",
    "| **ML Gate** | median RankIC (tabular_lgb) | \u2265 0.05 | ML extracts signal beyond factors |\n",
    "| **Model Gate** | median RankIC (model) | \u2265 ML baseline + 0.02 | Deep learning adds value |\n",
    "\n",
    "**Gating policy**:\n",
    "- Factor gate must pass before proceeding (confirms data quality)\n",
    "- ML gate establishes the \"ML floor\" for TSFM models\n",
    "- TSFM models must beat tuned ML baseline on **median OOS RankIC**\n",
    "\n",
    "---\n",
    "\n",
    "### 7.3 Running Baselines\n",
    "\n",
    "```python\n",
    "from pathlib import Path\n",
    "from src.evaluation import (\n",
    "    ExperimentSpec,\n",
    "    run_experiment,\n",
    "    SMOKE_MODE,\n",
    "    FULL_MODE,\n",
    "    FACTOR_BASELINES,\n",
    "    SANITY_BASELINES,\n",
    ")\n",
    "\n",
    "# Run all factor baselines (SMOKE mode for CI)\n",
    "for baseline_name in FACTOR_BASELINES:\n",
    "    spec = ExperimentSpec.baseline(baseline_name, cadence=\"monthly\")\n",
    "    results = run_experiment(\n",
    "        experiment_spec=spec,\n",
    "        features_df=features,  # Your features DataFrame\n",
    "        output_dir=Path(\"evaluation_outputs\"),\n",
    "        mode=SMOKE_MODE  # or FULL_MODE for production\n",
    "    )\n",
    "    print(f\"{baseline_name}: {results['n_folds']} folds\")\n",
    "\n",
    "# Run sanity baseline (verify ~0 RankIC)\n",
    "spec = ExperimentSpec.baseline(\"naive_random\", cadence=\"monthly\")\n",
    "sanity_results = run_experiment(spec, features, Path(\"evaluation_outputs\"), SMOKE_MODE)\n",
    "assert abs(sanity_results[\"median_rankic\"]) < 0.05, \"Pipeline sanity failed!\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 7.4 Acceptance Criteria (Models Must Clear)\n",
    "\n",
    "| Criterion | Threshold | Rationale |\n",
    "|-----------|-----------|-----------|\n",
    "| **RankIC Lift** | Model median RankIC >= best baseline + 0.02 | ML must add meaningful signal |\n",
    "| **Net-Positive Folds** | >= 70% of folds positive after base costs | Signal must survive realistic trading |\n",
    "| **Top-10 Churn** | Median < 30% | Rankings must be stable enough to trade |\n",
    "| **No Collapse** | 0 folds with negative median RankIC | Robust across regimes |\n",
    "\n",
    "These criteria are computed via:\n",
    "```python\n",
    "from src.evaluation import compute_acceptance_verdict, save_acceptance_summary\n",
    "\n",
    "verdict = compute_acceptance_verdict(\n",
    "    model_summary,\n",
    "    baseline_summaries={\n",
    "        \"mom_12m\": ..., \n",
    "        \"momentum_composite\": ..., \n",
    "        \"short_term_strength\": ...,\n",
    "        \"tabular_lgb\": ...  # Once implemented\n",
    "    },\n",
    "    cost_overlays=cost_df,\n",
    "    churn_df=churn_df\n",
    ")\n",
    "save_acceptance_summary(verdict, Path(\"outputs\"), \"model_vs_baselines\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 7.5 FULL_MODE Reference Run (Required Before Model Work)\n",
    "\n",
    "**Before training any model**, freeze a FULL_MODE baseline run:\n",
    "- **Range**: 2016-01-01 \u2192 2025-06-30 (locked)\n",
    "- **Cadence**: Monthly (primary), Quarterly (robustness)\n",
    "- **Outputs**: All baselines, all 3 horizons (20/60/90)\n",
    "\n",
    "This produces:\n",
    "```\n",
    "evaluation_outputs/\n",
    "\u251c\u2500\u2500 baseline_mom_12m_monthly/\n",
    "\u2502   \u251c\u2500\u2500 eval_rows.parquet\n",
    "\u2502   \u251c\u2500\u2500 per_date_metrics.csv\n",
    "\u2502   \u251c\u2500\u2500 fold_summaries.csv\n",
    "\u2502   \u251c\u2500\u2500 cost_overlays.csv\n",
    "\u2502   \u251c\u2500\u2500 stability_scorecard.csv\n",
    "\u2502   \u2514\u2500\u2500 REPORT_SUMMARY.md\n",
    "\u251c\u2500\u2500 baseline_momentum_composite_monthly/\n",
    "\u251c\u2500\u2500 baseline_short_term_strength_monthly/\n",
    "\u251c\u2500\u2500 baseline_naive_random_monthly/\n",
    "\u251c\u2500\u2500 baseline_tabular_lgb_monthly/  # Once implemented\n",
    "\u2514\u2500\u2500 BASELINE_REFERENCE.md  # Frozen floor for all horizons\n",
    "```\n",
    "\n",
    "**Freeze requirements**:\n",
    "- Git commit hash of the run\n",
    "- Run configuration (cadence, horizons, eval range, cost scenarios)\n",
    "- Data snapshot identity (DuckDB hash + row counts)\n",
    "- Output directory + manifest\n",
    "- Environment snapshot (Python version + pip freeze)\n",
    "\n",
    "---\n",
    "\n",
    "### 7.6 Tabular ML Baseline \u2705 IMPLEMENTED + FROZEN\n",
    "\n",
    "The ML baseline uses LightGBM Regressor on the same feature stack as factor baselines.\n",
    "\n",
    "#### 7.6.1 Training Protocol (Implemented)\n",
    "\n",
    "```python\n",
    "# Per-fold training using walk-forward split\n",
    "for fold in walk_forward_folds:\n",
    "    # Uses same purging + embargo + maturity\n",
    "    train_data = fold.train_df\n",
    "    val_data = fold.val_data\n",
    "    \n",
    "    # Horizon-specific models (separate model per horizon)\n",
    "    for horizon in [20, 60, 90]:\n",
    "        model = lgb.LGBMRegressor(  # Regressor (not Ranker) for continuous returns\n",
    "            objective=\"regression_l1\",\n",
    "            metric=\"rmse\",\n",
    "            n_estimators=100,  # FROZEN after initial tuning\n",
    "            learning_rate=0.05,\n",
    "            max_depth=5,\n",
    "            num_leaves=31,\n",
    "            random_state=42,\n",
    "            # Time-decay weighting: recent data weighted higher (half-life=252d)\n",
    "        )\n",
    "        model.fit(\n",
    "            train_data[features],\n",
    "            train_data[f\"excess_return_{horizon}d\"],\n",
    "            group=train_data.groupby(\"as_of_date\").size().values,\n",
    "            sample_weight=time_decay_weights  # Exponential decay\n",
    "        )\n",
    "```\n",
    "\n",
    "#### 7.6.2 Frozen Baseline Floor (COMPLETE)\n",
    "\n",
    "**Status:** FROZEN at tag `chapter7-tabular-lgb-freeze`  \n",
    "**Artifacts:** `evaluation_outputs/chapter7_tabular_lgb_full/`  \n",
    "**Reference:** `BASELINE_REFERENCE.md` + `CLOSURE_MANIFEST.json`\n",
    "\n",
    "| Horizon | Frozen Factor Floor | tabular_lgb (ML) | Lift |\n",
    "|---------|---------------------|------------------|------|\n",
    "| 20d | 0.0283 | **0.1009** | **+0.0726** |\n",
    "| 60d | 0.0392 | **0.1275** | **+0.0883** |\n",
    "| 90d | 0.0169 | **0.1808** | **+0.1639** |\n",
    "\n",
    "**Implementation complete:**\n",
    "1. \u2705 One-time param tuning completed\n",
    "2. \u2705 Params frozen and recorded\n",
    "3. \u2705 FULL_MODE reference run executed (109 monthly folds, 36 quarterly)\n",
    "4. \u2705 Artifacts frozen with git tag\n",
    "5. \u2705 **ML baseline is now immutable**\n",
    "\n",
    "---\n",
    "\n",
    "### 7.7 Output Schema (EvaluationRow Contract)\n",
    "\n",
    "Every baseline (and model) produces rows in this exact format:\n",
    "\n",
    "| Field | Type | Description |\n",
    "|-------|------|-------------|\n",
    "| `as_of_date` | date | Evaluation date |\n",
    "| `ticker` | str | Stock symbol |\n",
    "| `stable_id` | str | Survivorship-safe identifier |\n",
    "| `horizon` | int | 20, 60, or 90 trading days |\n",
    "| `fold_id` | str | Walk-forward fold |\n",
    "| `score` | float | Ranking score (HIGHER = BETTER) |\n",
    "| `excess_return` | float | v2 total return vs benchmark |\n",
    "| `adv_20d` | float | Average daily volume (cost realism) |\n",
    "\n",
    "**Rules**:\n",
    "- No duplicates per (as_of_date, stable_id, horizon)\n",
    "- Missing score/return \u2192 row dropped (logged)\n",
    "- Tie-breaking: deterministic via stable_id\n",
    "\n",
    "---\n",
    "\n",
    "### 7.8 Implementation Location\n",
    "\n",
    "```\n",
    "src/evaluation/baselines.py     # Baseline definitions and scoring\n",
    "src/evaluation/run_evaluation.py # End-to-end runner (SMOKE/FULL modes)\n",
    "tests/test_baselines.py         # 39 tests (monotonicity, determinism, etc.)\n",
    "tests/test_end_to_end_smoke.py  # 22 integration tests\n",
    "```\n",
    "\n",
    "**Tests**: 61 tests specific to baselines and runner, all passing.\n",
    "\n",
    "---\n",
    "\n",
    "### 7.9 What \"Done\" Looks Like\n",
    "\n",
    "When Chapter 7 baseline work is complete:\n",
    "\n",
    "```\n",
    "evaluation_outputs/\n",
    "\u251c\u2500\u2500 baseline_mom_12m_monthly/\n",
    "\u251c\u2500\u2500 baseline_mom_12m_quarterly/\n",
    "\u251c\u2500\u2500 baseline_momentum_composite_monthly/\n",
    "\u251c\u2500\u2500 baseline_momentum_composite_quarterly/\n",
    "\u251c\u2500\u2500 baseline_short_term_strength_monthly/\n",
    "\u251c\u2500\u2500 baseline_short_term_strength_quarterly/\n",
    "\u251c\u2500\u2500 baseline_naive_random_monthly/  # Sanity check\n",
    "\u251c\u2500\u2500 baseline_tabular_lgb_monthly/   # ML baseline\n",
    "\u251c\u2500\u2500 baseline_tabular_lgb_quarterly/\n",
    "\u2514\u2500\u2500 BASELINE_REFERENCE.md\n",
    "```\n",
    "\n",
    "The `BASELINE_REFERENCE.md` contains:\n",
    "- Median RankIC per horizon (20/60/90), monthly and quarterly\n",
    "- Churn and cost-survival diagnostics\n",
    "- Pass/fail for each baseline gate\n",
    "- Frozen commit hash + output path\n",
    "\n",
    "**This becomes the only reference point for Chapter 8+ (Kronos, FinText, Fusion).**\n",
    "\n",
    "---\n",
    "\n",
    "### DEV / FINAL Holdout Protocol (established Feb 19, 2026)\n",
    "\n",
    "All evaluation is partitioned into two windows:\n",
    "\n",
    "| Window | Date Range | Months | Purpose |\n",
    "|--------|-----------|:------:|---------|\n",
    "| **DEV** | Feb 2016 \u2013 Dec 2023 | 95 | Research iteration |\n",
    "| **FINAL** | Jan 2024 \u2013 Feb 2025 | 14 | One-shot confirmation |\n",
    "\n",
    "**Cutoff:** `HOLDOUT_START = 2024-01-01`\n",
    "\n",
    "**Retroactive caveat:** This split was established after development iteration\n",
    "on the full 109-fold aggregate. It is a soft holdout, not a true blind holdout.\n",
    "\n",
    "**Key finding:** LGB signal collapses at 60d/90d in the holdout (RankIC goes\n",
    "negative) but 20d retains weak positive signal (0.010). Shadow portfolio at 20d\n",
    "holds Sharpe 1.91 in holdout (vs 3.15 DEV). Year-by-year analysis shows regime\n",
    "dependency \u2014 model fails in strong thematic bull markets (2021, 2024) \u2014 not\n",
    "pure overfitting.\n",
    "\n",
    "**Rules:**\n",
    "1. All chapters report both **DEV** and **FINAL** metrics\n",
    "2. Model iteration uses **DEV only**\n",
    "3. FINAL evaluated **once per chapter** as confirmation\n",
    "4. 20d is the **confirmed primary horizon** (others are DEV-only until confirmed)\n",
    "\n",
    "See `documentation/CHAPTER_12.md` Appendix and `documentation/ROADMAP.md` for\n",
    "full analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Kronos Module (Price Dynamics)\n",
    "\n",
    "### 8.0 Scope (Keep Chapter 8 Signal-Researcher Grade)\n",
    "\n",
    "Chapter 8 is about **integrating Kronos** cleanly into the **frozen** evaluation pipeline and validating signal quality via:\n",
    "- **RankIC / IC stability**\n",
    "- **Quintile spread**\n",
    "- **Churn / turnover proxies**\n",
    "- **Net-of-cost survival** (diagnostic overlays)\n",
    "- **Regime slices**\n",
    "- **PIT + survivorship correctness**\n",
    "\n",
    "We intentionally do **not** introduce full portfolio backtests (Sharpe/IR/DD) here. That belongs later (see Chapter 11).\n",
    "\n",
    "---\n",
    "\n",
    "### 8.1 Kronos Input/Output (Locked)\n",
    "\n",
    "- **Input**: OHLCV sequences (from DuckDB `prices` table via `PricesStore`)\n",
    "- **Future timestamps**: derived from **global trading calendar** (DuckDB dates) \u2014 **never** `freq=\"B\"`\n",
    "- **Inference path**: **batch-first** (`predict_batch`) with deterministic SMOKE settings\n",
    "- **Score mapping (ranking proxy)**:\n",
    "\n",
    "$$\n",
    "score_{t,h} = \\frac{\\hat{C}_{t+h} - C_t}{C_t}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $C_t$ is close at as-of date \\(t\\) (from `PricesStore`)\n",
    "-$hat{C}_{t+h}$ is predicted close at horizon $h$ (from Kronos)\n",
    "\n",
    "---\n",
    "\n",
    "### 8.2 Kronos Success Criteria (Signal Quality)\n",
    "\n",
    "**Minimum gates (signal-only):**\n",
    "- Runs end-to-end with correct scorer contract (scores for every `(date, ticker)` in validation)\n",
    "- RankIC is meaningfully positive for at least one horizon (primary focus: **20d**)\n",
    "- Churn/cost overlays do not destroy the signal\n",
    "- Performance is not purely a momentum clone (correlation checks)\n",
    "\n",
    "---\n",
    "\n",
    "### 8.3 Kronos Leak Tripwires (Evaluation-only, Cheap Insurance)\n",
    "\n",
    "Because strong metrics can still hide subtle alignment bugs, add **two negative controls** as a leak tripwire:\n",
    "\n",
    "1) **Shuffle-within-date**\n",
    "- Shuffle scores **within each date** (cross-section)\n",
    "- Expectation: **RankIC \u2248 0**\n",
    "\n",
    "2) **+1 trading-day lag**\n",
    "- Shift scores forward by **+1 trading day** (or equivalently score date misalignment)\n",
    "- Expectation: RankIC **collapses materially**\n",
    "\n",
    "**Notes:**\n",
    "- These are evaluation-only checks; they do **not** change model training.\n",
    "- They should be implemented as part of **Phase 3+ (evaluation integration)**, not Phase 2.\n",
    "\n",
    "---\n",
    "\n",
    "### 8.4 What We Do NOT Add in Chapter 8\n",
    "\n",
    "- No canonical Sharpe/IR/DD yet (that\u2019s Chapter 11+)\n",
    "- No optimizer/execution\n",
    "- No overlapping-hold portfolio logic for 60/90 horizons\n",
    "\n",
    "(Keep Chapter 8 focused: correctness + scalable integration + signal research metrics.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9) FinText-TSFM Module (Return Structure)\n",
    "\n",
    "### 9.0 Scope & Context\n",
    "\n",
    "Chapter 9 integrates **FinText-TSFM** \u2014 a suite of 600+ time series foundation models pre-trained from scratch on financial excess returns \u2014 into our frozen evaluation pipeline.\n",
    "\n",
    "**Why FinText after Kronos?**\n",
    "Chapter 8 showed that generic TSFM (Kronos, pre-trained on OHLCV across exchanges) produces inverted rankings zero-shot. The [FinText paper](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5770562) (Rahimikia, Ni & Wang, 2025) demonstrates that **finance-native pre-training** on excess returns dramatically outperforms generic foundation models \u2014 exactly the failure mode we observed with Kronos.\n",
    "\n",
    "**Key advantages over Kronos:**\n",
    "- **Input = daily excess returns** (not OHLCV) \u2192 directly matches our label definition\n",
    "- **Pre-trained on 2B+ financial observations** across 94 markets (not generic time series)\n",
    "- **Year-specific models** \u2192 inherently PIT-safe (no look-ahead bias by construction)\n",
    "- **Proven academic results** with proper out-of-sample evaluation\n",
    "\n",
    "**References:**\n",
    "- Paper: \"Re(Visiting) Time Series Foundation Models in Finance\" ([SSRN 5770562](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5770562))\n",
    "- Models: [HuggingFace/FinText](https://huggingface.co/FinText) (613 models)\n",
    "- Code: [GitHub/DeepIntoStreams/TSFM_Finance](https://github.com/DeepIntoStreams/TSFM_Finance)\n",
    "\n",
    "---\n",
    "\n",
    "### 9.1 FinText Input / Output (Locked)\n",
    "\n",
    "**Input:** Historical daily excess return sequences (PIT-safe)\n",
    "- Source: DuckDB `prices` table + QQQ benchmark prices\n",
    "- Daily excess return: `r_stock_t - r_QQQ_t` where `r = (close_t / close_{t-1}) - 1`\n",
    "- Lookback window: **21 trading days** (primary), 252d and 512d (ablation)\n",
    "- Shape: `torch.Tensor` of shape `(n_stocks, lookback_window)`\n",
    "\n",
    "**Model:** Amazon Chronos architecture, finance-pre-trained by FinText team\n",
    "- Primary: `FinText/Chronos_Small_{YEAR}_US` (46M params, U.S. excess returns)\n",
    "- Secondary ablations: `Chronos_Mini` (20M), `Chronos_Tiny` (8M), `Global`, `Augmented`\n",
    "- Year selection (PIT-safe): For as-of date in year Y, load model trained through Y-1\n",
    "  - Example: evaluating at 2024-03-01 \u2192 use `FinText/Chronos_Small_2023_US`\n",
    "  - Example: evaluating at 2020-06-15 \u2192 use `FinText/Chronos_Small_2019_US`\n",
    "\n",
    "**Output:** Predicted future excess return distribution\n",
    "- `num_samples` draws from the predictive distribution (default: 20)\n",
    "- Shape: `(n_stocks, num_samples, prediction_length)`\n",
    "- prediction_length = 1 (next-day excess return; aggregate for multi-day horizons)\n",
    "\n",
    "**Score mapping (ranking proxy):**\n",
    "\n",
    "$$\n",
    "\\text{score}_{i,t} = \\text{median}\\left(\\hat{r}^{(1)}_{i,t+1}, \\ldots, \\hat{r}^{(S)}_{i,t+1}\\right)\n",
    "$$\n",
    "\n",
    "Where $\\hat{r}^{(s)}_{i,t+1}$ is sample $s$ of the predicted excess return for stock $i$.\n",
    "\n",
    "**Uncertainty quantification (for Chapter 13):**\n",
    "- Spread: IQR of samples \u2192 natural confidence measure\n",
    "- Save full sample distribution for calibration in Chapter 13\n",
    "\n",
    "---\n",
    "\n",
    "### 9.2 Data Plumbing: Daily Excess Return Sequences\n",
    "\n",
    "**New component:** `src/data/excess_return_store.py`\n",
    "\n",
    "Responsibilities:\n",
    "1. Load stock daily closes from DuckDB `prices` table\n",
    "2. Load QQQ daily closes (add to DuckDB `prices` table if not present)\n",
    "3. Compute daily excess returns: `stock_daily_return - QQQ_daily_return`\n",
    "4. Return windowed sequences for any `(ticker, asof_date, lookback)`\n",
    "5. Cache computed sequences for efficiency\n",
    "\n",
    "**PIT rules (same as Chapter 8):**\n",
    "- Only use data with date \u2264 asof_date\n",
    "- All returns computed from split-adjusted prices\n",
    "- QQQ must be in the same time grid (NYSE trading days)\n",
    "\n",
    "**QQQ benchmark data:**\n",
    "- Already cached from FMP: `data/cache/fmp/...symbol=QQQ...`\n",
    "- One-time script to add QQQ to DuckDB `prices` table\n",
    "- Verify date alignment with stock universe\n",
    "\n",
    "---\n",
    "\n",
    "### 9.3 FinText Adapter\n",
    "\n",
    "**New component:** `src/models/fintext_adapter.py`\n",
    "\n",
    "```python\n",
    "class FinTextAdapter:\n",
    "    \"\"\"\n",
    "    Adapter for FinText-TSFM (Chronos) foundation model inference.\n",
    "    \n",
    "    Key design decisions:\n",
    "    1. Uses ExcessReturnStore for input sequences (daily excess returns)\n",
    "    2. Year-aware model loading (PIT-safe)\n",
    "    3. Batch inference via ChronosPipeline\n",
    "    4. Score = median predicted excess return\n",
    "    \"\"\"\n",
    "    \n",
    "    # Year-aware model selection\n",
    "    def get_model_id(self, asof_date) -> str:\n",
    "        year = asof_date.year - 1  # Use model trained through previous year\n",
    "        year = min(year, 2023)     # Latest available model\n",
    "        year = max(year, 2000)     # Earliest available model\n",
    "        return f\"FinText/Chronos_Small_{year}_US\"\n",
    "    \n",
    "    # Core inference\n",
    "    def score_universe_batch(\n",
    "        self,\n",
    "        asof_date: pd.Timestamp,\n",
    "        tickers: List[str],\n",
    "        horizon: int,\n",
    "        lookback: int = 21,\n",
    "        num_samples: int = 20,\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Score all tickers for one as-of date.\"\"\"\n",
    "```\n",
    "\n",
    "**Model caching strategy:**\n",
    "- Cache loaded models in memory (keyed by model_id)\n",
    "- Walk-forward evaluation processes dates chronologically \u2192 same model reused for months within a year\n",
    "- Memory footprint: ~200MB per Chronos-Small model\n",
    "\n",
    "**Dependencies:**\n",
    "- `chronos-forecasting>=1.5` (Amazon Chronos)\n",
    "- `transformers>=4.35` (already in requirements)\n",
    "- `torch>=2.0` (already in requirements)\n",
    "\n",
    "---\n",
    "\n",
    "### 9.4 Multi-Day Horizon Strategy\n",
    "\n",
    "FinText Chronos predicts **1-day-ahead** excess returns. Our evaluation uses 20/60/90 trading day horizons.\n",
    "\n",
    "**Approach: Autoregressive multi-step (primary)**\n",
    "- For horizon H, run the model autoregressively H steps\n",
    "- Use `prediction_length=H` in `pipeline.predict()` (Chronos handles internal AR)\n",
    "- Score = median of predicted return at step H\n",
    "\n",
    "**Alternative: Cumulative sum (ablation)**\n",
    "- Predict H daily returns, sum them for cumulative excess return\n",
    "- `cum_score = sum(median_prediction[1..H])`\n",
    "- May capture path effects better\n",
    "\n",
    "**Start with:** `prediction_length=1` (simplest, cleanest) and use the single-step predicted return as the ranking score for all horizons. This is justified because:\n",
    "- Cross-sectional ranking is relative (absolute magnitude doesn't matter)\n",
    "- If a stock's 1-day predicted excess return is positive, its multi-day return tends to be too\n",
    "- Reduces inference cost by 20-90x vs full autoregressive unrolling\n",
    "\n",
    "Test multi-step as an ablation if 1-step shows promising RankIC.\n",
    "\n",
    "---\n",
    "\n",
    "### 9.5 Walk-Forward Model Selection (PIT-Safe)\n",
    "\n",
    "Unlike Kronos (single pre-trained model), FinText provides year-specific models.\n",
    "\n",
    "**Walk-forward model assignment:**\n",
    "\n",
    "| Evaluation Period | Model Used | Rationale |\n",
    "|-------------------|------------|-----------|\n",
    "| 2016-01 to 2016-12 | `Chronos_Small_2015_US` | Trained through 2015 |\n",
    "| 2017-01 to 2017-12 | `Chronos_Small_2016_US` | Trained through 2016 |\n",
    "| ... | ... | ... |\n",
    "| 2024-01 to 2025-06 | `Chronos_Small_2023_US` | Latest model (trained through 2023) |\n",
    "\n",
    "This is strictly PIT-safe: each model only uses data available before the evaluation period.\n",
    "\n",
    "---\n",
    "\n",
    "### 9.6 FinText Success Criteria (Signal Quality)\n",
    "\n",
    "**Minimum gates (same framework as Chapter 8):**\n",
    "\n",
    "| Gate | Metric | Threshold | Rationale |\n",
    "|------|--------|-----------|-----------|\n",
    "| Gate 1 (Factor) | RankIC \u2265 0.02 for \u22652 horizons | Factor baseline | Confirms signal exists |\n",
    "| Gate 2 (ML) | Any horizon RankIC \u2265 0.05 or within 0.03 of LGB | ML comparison | Worth pursuing for fusion |\n",
    "| Gate 3 (Practical) | Churn \u2264 30%, stable across regimes | Implementability | Rankings must be tradeable |\n",
    "\n",
    "**Additional success criteria:**\n",
    "- Low correlation with `tabular_lgb` scores (< 0.5) \u2192 complementary signal for fusion\n",
    "- Low correlation with Kronos scores \u2192 orthogonal view\n",
    "- Stable IC sign across \u2265 70% of rolling windows\n",
    "\n",
    "**Comparison targets (from frozen baselines):**\n",
    "\n",
    "| Horizon | Factor Floor | ML Floor (tabular_lgb) | FinText Target |\n",
    "|---------|-------------|----------------------|----------------|\n",
    "| 20d | 0.0283 | 0.1009 | \u2265 0.02 (Gate 1) |\n",
    "| 60d | 0.0392 | 0.1275 | \u2265 0.05 (Gate 2) |\n",
    "| 90d | 0.0169 | 0.1808 | \u2265 0.02 (Gate 1) |\n",
    "\n",
    "**Note:** Even if FinText doesn't beat LGB on RankIC, a weakly positive but **orthogonal** signal is valuable for fusion (Chapter 11).\n",
    "\n",
    "---\n",
    "\n",
    "### 9.7 Leak Tripwires (Evaluation-Only)\n",
    "\n",
    "Reuse the same negative controls used for Kronos:\n",
    "\n",
    "1. **Shuffle-within-date** \u2192 RankIC \u2248 0\n",
    "   - Shuffle scores within each date (cross-section)\n",
    "   - Confirms signal is stock-specific, not systematic\n",
    "\n",
    "2. **+1 trading-day lag** \u2192 RankIC collapses\n",
    "   - Shift scores forward by 1 trading day\n",
    "   - Confirms signal is time-aligned\n",
    "\n",
    "3. **Year-mismatch control** (NEW for FinText)\n",
    "   - Use deliberately wrong model year (e.g., 2023 model for 2018 dates)\n",
    "   - If RankIC doesn't change \u2192 model year doesn't matter (suspicious)\n",
    "   - If RankIC degrades \u2192 year-specific training provides genuine value\n",
    "\n",
    "---\n",
    "\n",
    "### 9.8 Ablation Studies\n",
    "\n",
    "| Ablation | What We Learn |\n",
    "|----------|--------------|\n",
    "| **Lookback window**: 21 vs 252 vs 512 | Optimal context length for ranking |\n",
    "| **Model size**: Tiny (8M) vs Mini (20M) vs Small (46M) | Quality vs speed tradeoff |\n",
    "| **Model dataset**: US vs Global vs Augmented | Domain specificity impact |\n",
    "| **Prediction length**: 1-step vs H-step autoregressive | Multi-day horizon strategy |\n",
    "| **Score aggregation**: Median vs mean vs trimmed mean | Robustness of score mapping |\n",
    "| **Num samples**: 5 vs 20 vs 50 | Distribution estimation stability |\n",
    "\n",
    "Run ablations in SMOKE mode first; only promote to FULL for promising variants.\n",
    "\n",
    "---\n",
    "\n",
    "### 9.9 Implementation Phases\n",
    "\n",
    "**Phase 1: Data Plumbing (\u00bd day)**\n",
    "1. Add QQQ to DuckDB `prices` table (from existing FMP cache)\n",
    "2. Implement `ExcessReturnStore` (daily excess return sequences)\n",
    "3. Unit tests for return computation and PIT safety\n",
    "4. Verify: sequences match manual calculation for sample dates\n",
    "\n",
    "**Phase 2: FinText Adapter (1 day)**\n",
    "1. Install `chronos-forecasting` package\n",
    "2. Implement `FinTextAdapter` with year-aware model loading\n",
    "3. Implement `fintext_scoring_function()` for `run_experiment()` integration\n",
    "4. Stub mode for testing without model download\n",
    "5. Single-stock sanity test\n",
    "6. Unit tests (15+ tests covering all non-negotiables)\n",
    "\n",
    "**Phase 3: Evaluation Integration (\u00bd day)**\n",
    "1. Create `scripts/run_chapter9_fintext.py`\n",
    "2. SMOKE evaluation (3 folds, verify pipeline)\n",
    "3. Leak tripwires (shuffle + lag + year-mismatch)\n",
    "4. Momentum-clone check (correlation vs factor baselines)\n",
    "\n",
    "**Phase 4: Full Evaluation & Ablation (1 day)**\n",
    "1. FULL mode evaluation (109 monthly folds, 3 horizons)\n",
    "2. Compare vs frozen baselines (factor + ML)\n",
    "3. Run primary ablations (lookback, model size)\n",
    "4. Document results and gate evaluation\n",
    "5. Freeze Chapter 9 artifacts (if gates pass)\n",
    "\n",
    "---\n",
    "\n",
    "### 9.10 Files to Create\n",
    "\n",
    "```\n",
    "src/data/excess_return_store.py        # Daily excess return sequences\n",
    "src/models/fintext_adapter.py          # FinText-TSFM adapter\n",
    "scripts/add_qqq_to_duckdb.py           # One-time: add QQQ prices\n",
    "scripts/run_chapter9_fintext.py        # Walk-forward evaluation\n",
    "scripts/test_fintext_single_stock.py   # Quick sanity check\n",
    "tests/test_excess_return_store.py      # Unit tests for data\n",
    "tests/test_fintext_adapter.py          # Unit tests for adapter\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 9.11 What We Do NOT Add in Chapter 9\n",
    "\n",
    "- No Sharpe/IR/DD yet (that's Chapter 11+)\n",
    "- No optimizer/execution\n",
    "- No overlapping-hold portfolio logic\n",
    "- No fine-tuning of FinText models (zero-shot only; the models are already finance-pre-trained)\n",
    "- No fusion with tabular features (that's Chapter 11)\n",
    "\n",
    "(Keep Chapter 9 focused: correctness + scalable integration + signal research metrics.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10) NLP Sentiment Signal\n",
    "\n",
    "**Goal:** Add a text-based sentiment signal that is **orthogonal** to price/fundamental features, for use in Chapter 11 fusion.\n",
    "\n",
    "**Why this matters for fusion:** Our existing features (Ch5) capture price dynamics, fundamentals, and macro regime. Sentiment captures **information flow** \u2014 what people are saying about a stock before it's fully reflected in prices. For AI stocks (news-heavy sector), this is especially valuable.\n",
    "\n",
    "**Model:** ProsusAI/FinBERT (pre-trained finance sentiment, zero-shot, no fine-tuning needed \u2014 same philosophy as FinText in Ch9).\n",
    "\n",
    "**Data Sources:**\n",
    "- **SEC EDGAR 8-K filings** (free, unlimited, PIT-safe): Material event disclosures, earnings releases, management commentary\n",
    "- **FinnHub company news API** (free tier, 60 req/min): Real-time news headlines with timestamps for our 100 AI stocks\n",
    "- **Existing EventStore** infrastructure (Ch4): Already supports `SENTIMENT` event type with PIT timestamps\n",
    "\n",
    "---\n",
    "\n",
    "### 10.1 Sentiment Data Pipeline\n",
    "\n",
    "**SEC EDGAR 8-K Text Extraction:**\n",
    "- Download 8-K filing text for all 100 AI stocks (2016\u20132025)\n",
    "- Extract key sections: Item 2.02 (Results of Operations), Item 7.01 (Regulation FD), Item 8.01 (Other Events)\n",
    "- PIT enforcement: use `acceptanceDateTime` from EDGAR (to-the-second accuracy)\n",
    "- Rate limit: 8 req/sec (EDGAR guideline), no daily cap\n",
    "\n",
    "**FinnHub News Collection:**\n",
    "- Collect company-specific news for all 100 AI stocks\n",
    "- Fields: headline, summary, datetime, source, category\n",
    "- PIT enforcement: use article `datetime` as observed_at\n",
    "- Rate limit: 60 req/min (free tier)\n",
    "- **Requires:** Free FinnHub API key (https://finnhub.io/register)\n",
    "\n",
    "**Storage:**\n",
    "- Store raw text + metadata in `EventStore` with `event_type='SENTIMENT'`\n",
    "- Deduplicate by source + datetime + ticker\n",
    "- Cache locally to avoid re-downloading\n",
    "\n",
    "**Non-negotiables:**\n",
    "- Every text record has a PIT-safe timestamp\n",
    "- No future text leaks into historical scoring\n",
    "- Text is stored raw (scoring happens separately)\n",
    "\n",
    "---\n",
    "\n",
    "### 10.2 FinBERT Sentiment Scoring\n",
    "\n",
    "**Model:** `ProsusAI/finbert` (HuggingFace, ~110M params)\n",
    "- Pre-trained on financial text (news, reports, SEC filings)\n",
    "- Outputs: P(positive), P(negative), P(neutral) per text chunk\n",
    "- Zero-shot \u2014 no fine-tuning required\n",
    "\n",
    "**Scoring Pipeline:**\n",
    "1. For each text record (8-K excerpt or news headline+summary):\n",
    "   - Tokenize with FinBERT tokenizer (max 512 tokens)\n",
    "   - For long filings: split into chunks, score each, aggregate (mean of top-3 most opinionated chunks)\n",
    "   - Output: `sentiment_score` = P(positive) - P(negative) \u2208 [-1, +1]\n",
    "2. Store scored results back in EventStore with original PIT timestamp\n",
    "3. Batch processing: score all historical text in one pass, then incremental updates\n",
    "\n",
    "**Quality Checks:**\n",
    "- Verify score distribution is not degenerate (not all neutral)\n",
    "- Spot-check known events (e.g., NVDA earnings beat \u2192 should be positive)\n",
    "- Cross-validate against known market reactions\n",
    "\n",
    "---\n",
    "\n",
    "### 10.3 Sentiment Feature Engineering\n",
    "\n",
    "Build per-stock sentiment features for each evaluation date, using only PIT-safe data:\n",
    "\n",
    "**Filing Sentiment Features (3):**\n",
    "- `filing_sentiment_latest`: Sentiment of most recent 8-K filing\n",
    "- `filing_sentiment_change`: Change in sentiment between last two filings\n",
    "- `filing_sentiment_90d`: Mean sentiment of all filings in past 90 days\n",
    "\n",
    "**News Sentiment Features (4):**\n",
    "- `news_sentiment_7d`: Mean sentiment of news in past 7 trading days\n",
    "- `news_sentiment_30d`: Mean sentiment of news in past 30 trading days\n",
    "- `news_sentiment_momentum`: 7d minus 30d sentiment (acceleration)\n",
    "- `news_volume_30d`: Count of news articles in past 30 days (attention proxy)\n",
    "\n",
    "**Cross-Sectional Features (2):**\n",
    "- `sentiment_zscore`: Cross-sectional z-score of 30d sentiment (rank within universe)\n",
    "- `sentiment_vs_momentum`: Residual of sentiment after regressing on price momentum (captures divergence between text and price signals)\n",
    "\n",
    "**Total: 9 new features** (deliberately small \u2014 avoid feature bloat, let fusion model decide weighting)\n",
    "\n",
    "**PIT Safety:** All features computed using only text observed before the evaluation date.\n",
    "\n",
    "---\n",
    "\n",
    "### 10.4 Walk-Forward Evaluation & Gates \u2705\n",
    "\n",
    "**SMOKE Results (3 folds, 2024):** 19,110 eval rows, 53 seconds.\n",
    "\n",
    "| Horizon | Mean RankIC | Churn | Gate 1 | Gate 2 | Gate 3 |\n",
    "|---------|-------------|-------|--------|--------|--------|\n",
    "| 20d | -0.015 | 10% | \u274c | \u274c | \u2705 |\n",
    "| 60d | -0.029 | 10% | \u274c | \u274c | \u2705 |\n",
    "| 90d | -0.066 | 10% | \u274c | \u274c | \u2705 |\n",
    "\n",
    "**Orthogonality (KEY RESULT):** \u03c1 < 0.16 vs ALL existing signals \u2192 HIGH fusion value.\n",
    "\n",
    "---\n",
    "\n",
    "### 10.5 Freeze & Documentation \u2705\n",
    "\n",
    "- 9 features frozen, artifacts saved to `evaluation_outputs/chapter10_sentiment_smoke/`\n",
    "- 101 tests passing (28 + 22 + 33 + 18)\n",
    "- CHAPTER_10.md + ROADMAP.md updated\n",
    "\n",
    "**Files to create:**\n",
    "```\n",
    "src/data/sentiment_store.py           # Sentiment data collection & storage\n",
    "src/models/finbert_scorer.py          # FinBERT sentiment scoring\n",
    "src/features/sentiment_features.py    # Sentiment feature engineering\n",
    "scripts/collect_sentiment_data.py     # One-time: collect historical sentiment\n",
    "scripts/run_chapter10_sentiment.py    # Walk-forward evaluation\n",
    "tests/test_sentiment_store.py         # Unit tests for data layer\n",
    "tests/test_finbert_scorer.py          # Unit tests for scoring\n",
    "tests/test_sentiment_features.py      # Unit tests for features\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11) Fusion Model (Ranking-First) \u2705 COMPLETE\n",
    "\n",
    "**Goal:** combine orthogonal signals into a robust cross-sectional ranking expert.\n",
    "\n",
    "**Signal families:** LGB (Ch7), FinText (Ch9, real Chronos), Sentiment (Ch10)\n",
    "\n",
    "### 11.1\u201311.4 \u2705 COMPLETE\n",
    "- Score alignment, fusion architecture (rank-avg, enriched LGB, stacking), gate evaluation, residual archive, expert interface\n",
    "- 36 tests passing, metric gap fixed (IC stability + cost survival added)\n",
    "\n",
    "### 11.5 FULL-Mode Comparison & Shadow Portfolio \u2705 COMPLETE\n",
    "\n",
    "All sub-models (including real Chronos FinText) re-run in FULL mode (109 folds, monthly).\n",
    "\n",
    "**Signal quality (109 folds, 2277 eval dates):**\n",
    "\n",
    "| Model | Hz | Med. RankIC | IC Stab | Cost Surv |\n",
    "|-------|---:|:----------:|:------:|:---------:|\n",
    "| **LGB baseline** | 20d | **0.0805** | **0.3534** | **66.8%** |\n",
    "| Learned Stacking | 20d | 0.0783 | 0.3452 | 65.9% |\n",
    "| Rank Avg 2 | 20d | 0.0538 | 0.2978 | 61.4% |\n",
    "| **LGB baseline** | 60d | **0.1478** | **0.7119** | **77.5%** |\n",
    "| Learned Stacking | 60d | 0.1480 | 0.7075 | 77.5% |\n",
    "| Rank Avg 2 | 60d | 0.0920 | 0.5610 | 70.4% |\n",
    "| **LGB baseline** | 90d | **0.1833** | **0.7972** | **79.8%** |\n",
    "| Learned Stacking | 90d | 0.1802 | 0.7961 | 79.9% |\n",
    "| Rank Avg 2 | 90d | 0.1173 | 0.6069 | 72.1% |\n",
    "\n",
    "**Shadow portfolio (20d):** LGB Sharpe **1.26** vs Stacking 1.14 vs RankAvg 0.84\n",
    "\n",
    "**FinText standalone:** near-zero signal (20d RankIC 0.014, 60d ~0, 90d slightly negative)\n",
    "\n",
    "### 11.6 Freeze \u2705 COMPLETE\n",
    "\n",
    "**Verdict:** Gate 4 FAIL \u2014 definitive with real FinText. LGB baseline wins on every metric. Root cause: FinText (Chronos) produces near-zero standalone signal for this AI stock universe. Learned Stacking matches LGB because Ridge learns to discard the weak sub-models. Infrastructure value delivered: residual archive, expert interface, disagreement proxy \u2014 all ready for Ch13 UQ.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12) Regime-Aware Analysis & Heuristic Ensemble \u2014 \u2705 COMPLETE\n",
    "\n",
    "**Goal:** Understand when LGB fails, test whether simple regime heuristics improve performance, and build the regime-based baseline that Chapter 13 DEUP must beat.\n",
    "\n",
    "**Result:** LGB performs 2.6\u00d7 better in calm markets (20d RankIC 0.18 vs 0.07). Vol-sizing heuristic provides modest improvement (Sharpe 2.65\u21922.73, max DD \u221222%\u2192\u221218%). Regime blending fails. Vol-sized LGB is the DEUP ablation baseline. `data/regime_context.parquet` frozen for Ch13 (201K rows, 16 features, 100% coverage). See `documentation/CHAPTER_12.md`.\n",
    "\n",
    "**Context from Chapter 11:** LGB baseline (Ch7) dominates all fusion variants. FinText has near-zero signal. There is no portfolio of strong models to ensemble \u2014 the value of Chapter 12 is *diagnostic* (when does our best model struggle?) and *infrastructure* (regime context for Ch13 UCB expert selection).\n",
    "\n",
    "**Role in UQ pipeline:** The UQ reference document (\u00a711.4) specifies four ablation baselines for Chapter 13's DEUP-based expert selection. One is **\"UCB with DEUP vs Regime-based switching (heuristic rules).\"** Chapter 12 builds that heuristic baseline. Additionally, the \"aleatoric\" half of dual-gated position sizing (\u00a77.5: `w = min(1, d / \u221a(a(x) + \u03b5))`) \u2014 using realized volatility \u2014 belongs here.\n",
    "\n",
    "**Existing infrastructure to reuse:**\n",
    "- `src/features/regime_features.py` \u2014 VIX regime (low/normal/elevated/high), market regime (bull/bear/neutral), sector rotation\n",
    "- `src/evaluation/metrics.py` \u2014 `evaluate_with_regime_slicing()`, `REGIME_DEFINITIONS` (VIX percentile, market regime, market vol)\n",
    "- `src/evaluation/reports.py` \u2014 `format_regime_performance()`, `plot_regime_bars()`\n",
    "- `scripts/run_shadow_portfolio.py` \u2014 Shadow portfolio (Sharpe, DD, turnover, hit rate)\n",
    "- LGB FULL eval_rows (591K rows, 109 folds, 2277 dates) already available\n",
    "\n",
    "---\n",
    "\n",
    "### 12.1 Regime-Conditional Performance Diagnostics\n",
    "\n",
    "Slice the LGB baseline across regime buckets to understand failure modes.\n",
    "\n",
    "**Work:**\n",
    "- Load LGB FULL eval_rows + annotate each `as_of_date` with regime features from `RegimeFeatureGenerator`\n",
    "- Compute per-regime: RankIC (median/mean), IC stability, cost survival, hit rate\n",
    "- Two regime axes: **VIX regime** (low/normal/elevated/high) and **market regime** (bull/bear/neutral)\n",
    "- Identify: does LGB struggle in high-VIX? In bear markets? At regime transitions?\n",
    "- Compute per-date RankIC correlation with VIX level and market return (continuous, not bucketed)\n",
    "\n",
    "**Output:** `evaluation_outputs/chapter12/regime_diagnostics.csv`, regime performance tables\n",
    "\n",
    "**Why it matters:** Foundation for everything downstream. DEUP (Ch13) needs to know *which features predict model failure* \u2014 regime is the most obvious candidate. Also essential for the thesis: \"Our model performs X in normal conditions and Y in stress conditions.\"\n",
    "\n",
    "---\n",
    "\n",
    "### 12.2 Regime Stress Tests (Shadow Portfolio)\n",
    "\n",
    "Extend the shadow portfolio with regime-conditional reporting.\n",
    "\n",
    "**Work:**\n",
    "- Annotate shadow portfolio daily returns with regime labels\n",
    "- Compute per-regime: annualized Sharpe, annualized return, max drawdown, hit rate\n",
    "- Plot rolling 12-month Sharpe with regime overlay (colored background)\n",
    "- Identify worst 5 drawdown episodes and their regime context\n",
    "- Compare: LGB vs Learned Stacking vs Rank Avg 2 by regime (using existing shadow portfolio returns)\n",
    "\n",
    "**Output:** `evaluation_outputs/chapter12/regime_stress_report.json`, rolling Sharpe plot\n",
    "\n",
    "**Why it matters:** Required for institutional-grade reporting. Shows if the Sharpe 1.26 is driven by one regime or is robust across conditions.\n",
    "\n",
    "---\n",
    "\n",
    "### 12.3 Regime-Aware Heuristic Baseline\n",
    "\n",
    "Build simple regime-conditioned approaches that serve as baselines for Ch13 DEUP.\n",
    "\n",
    "**Approach A \u2014 Volatility-Scaled Position Sizing:**\n",
    "- Scale signal by inverse realized volatility: `sized_score = score \u00d7 min(1, c / \u03c3_realized)`\n",
    "- The \"aleatoric\" half of dual-gated sizing from UQ reference \u00a77.5\n",
    "- Re-run shadow portfolio with sized scores, compare Sharpe/DD vs uniform\n",
    "\n",
    "**Approach B \u2014 Regime-Blended Ensemble:**\n",
    "- In normal/low-VIX regimes \u2192 use LGB scores as-is (full weight)\n",
    "- In elevated/high-VIX regimes \u2192 blend LGB with momentum factor (simpler, more robust model)\n",
    "- Smooth weights with exponential decay across regime boundaries (avoid hard switching artifacts)\n",
    "- Formula: `w_lgb = 1 - \u03b1 \u00d7 sigmoid((vix_pctile - 67) / \u03c4)`, where \u03b1 and \u03c4 are fixed (not optimized)\n",
    "\n",
    "**Evaluation:**\n",
    "- Run both through walk-forward evaluation (reuse `run_experiment()`)\n",
    "- Compare vs pure LGB: RankIC, IC stability, cost survival, churn\n",
    "- Shadow portfolio comparison: Sharpe, return, DD, worst month\n",
    "- SMOKE first (sanity), then FULL if either approach shows promise\n",
    "\n",
    "**Gate criteria:**\n",
    "- Does volatility sizing improve Sharpe or reduce max drawdown vs uniform?\n",
    "- Does regime blending improve any metric vs pure LGB?\n",
    "- If both fail \u2192 document negative result (still useful as Ch13 ablation baseline)\n",
    "\n",
    "**Output:** `evaluation_outputs/chapter12/regime_heuristic/` with eval_rows + shadow portfolio\n",
    "\n",
    "---\n",
    "\n",
    "### 12.4 Freeze & Documentation\n",
    "\n",
    "**Work:**\n",
    "- Compile final results table: LGB (uniform) vs vol-sized vs regime-blended\n",
    "- Store regime context features per-date for Ch13 (`data/regime_context.parquet`)\n",
    "  - Must include **per-stock realized volatility** (`vol_20d` from features store), not just market-level VIX\n",
    "  - DEUP's aleatoric baseline a(x) needs stock-level noise estimates for per-stock position sizing\n",
    "  - `vol_20d` already exists in `data/features.duckdb` (computed in Ch5 price features)\n",
    "- Document regime effects (which regimes hurt performance, by how much)\n",
    "- Write `documentation/CHAPTER_12.md`\n",
    "- Update `outline.ipynb` and `ROADMAP.md`\n",
    "\n",
    "**Success criteria (pragmatic):**\n",
    "- \u2705 Regime-conditional LGB performance quantified across all metrics\n",
    "- \u2705 Shadow portfolio stress-tested by regime with rolling analysis\n",
    "- \u2705 At least one heuristic baseline built for Ch13 DEUP ablation\n",
    "- \u2705 Regime context features stored and validated\n",
    "\n",
    "**Gate (Chapter 12-specific):**\n",
    "- If regime heuristics beat LGB \u2192 adopt as new primary signal\n",
    "- If regime heuristics \u2248 LGB \u2192 useful negative result; heuristic serves as Ch13 ablation baseline\n",
    "- Either outcome is valid \u2014 the key deliverable is the diagnostic analysis and the baseline\n",
    "\n",
    "**Estimated effort:** ~1 day (heavy reuse of existing evaluation + shadow portfolio infrastructure)\n",
    "\n",
    "### 12.5 What We Do NOT Add in Chapter 12\n",
    "\n",
    "- No HMM / Markov regime-switching models (overkill for ~50 stocks, and not needed before DEUP)\n",
    "- No optimized ensemble weights (would overfit 109 folds; Ch13 handles adaptive weighting properly)\n",
    "- No DEUP or epistemic uncertainty (that's Chapter 13)\n",
    "- No new model training (LGB is frozen from Ch7; we're testing regime-conditional *use* of existing models)\n",
    "- No execution logic (signal-only, evaluation-only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13) DEUP Uncertainty Quantification\n",
    "\n",
    "**Goal:** Implement DEUP (Direct Epistemic Uncertainty Prediction) for the AI Stock Forecaster expert. Train an error predictor g(x) on walk-forward residuals, calibrate an aleatoric baseline a(x), compute epistemic uncertainty e_hat(x) = max(0, g(x) - a(x)), and prove it adds economic value beyond simple volatility scaling AND beyond industry-standard UQ alternatives.\n",
    "\n",
    "**Primary horizon:** 20d (confirmed in holdout; 60d/90d are secondary).\n",
    "\n",
    "**DEUP test case:** Does e_hat(x) spike in Mar-Jul 2024 when the 20d signal goes negative? If yes, the system would have reduced positions during the AI rotation -- that's the core value proposition.\n",
    "\n",
    "**Ablation baseline:** Vol-sized LGB (Sharpe 2.73 ALL-period, from Ch12.3). DEUP must beat this.\n",
    "\n",
    "**All metrics reported as DEV (pre-2024) and FINAL (2024+) side by side.**\n",
    "\n",
    "---\n",
    "\n",
    "### 13.0 Populate Residual Archive & Loss Definition (infrastructure)\n",
    "\n",
    "**Loss definition (CRITICAL -- validated empirically):**\n",
    "\n",
    "The LGB model is a regressor (target = excess_return), but we evaluate it as a RANKER (RankIC). There are two candidate losses for g(x):\n",
    "\n",
    "| Loss | Definition | Corr with daily RankIC | vol_20d corr |\n",
    "|------|-----------|----------------------|-------------|\n",
    "| **Rank displacement** | |rank_pct(ER) - rank_pct(score)| per stock per date | rho = -0.974 | rho = 0.054 |\n",
    "| **MAE** | |excess_return - score| | rho = -0.119 | rho = 0.254 |\n",
    "\n",
    "**Decision: use rank displacement as the PRIMARY g(x) target.** Reasons:\n",
    "1. It correlates almost perfectly with RankIC (the metric we care about)\n",
    "2. vol_20d does NOT predict rank displacement (rho = 0.05) -- so e_hat will NOT be repackaged volatility\n",
    "3. Score std (0.025) is 4x smaller than ER std (0.104) -- raw MAE is dominated by irreducible return noise, not model error\n",
    "\n",
    "**Secondary: also train g(x) on MAE for comparison.** If the MAE-based DEUP produces similar results, the method is robust to loss choice.\n",
    "\n",
    "**Steps:**\n",
    "- Load LGB FULL eval_rows (591K rows, 109 folds, 3 horizons)\n",
    "- Compute per-date cross-sectional ranks for both scores and excess returns\n",
    "- Compute rank_loss = |rank_pct(ER) - rank_pct(score)| per (date, stock, horizon)\n",
    "- Also compute mae_loss = |ER - score| for secondary comparison\n",
    "- Store in `data/residuals.duckdb` via `ResidualArchive.save_from_eval_rows()`\n",
    "- Join regime_context features (vol_20d, vol_60d, mom_1m, vix_percentile_252d, market_regime, etc.) by (date, stable_id)\n",
    "- Also populate for Rank Avg 2 (844K rows, 109 folds)\n",
    "\n",
    "**Output:** `data/residuals.duckdb` populated with both loss types, enriched with regime features\n",
    "\n",
    "---\n",
    "\n",
    "### 13.1 Train g(x) Error Predictor\n",
    "\n",
    "g(x) predicts how wrong the primary model's RANKING will be at input x. Core of DEUP.\n",
    "\n",
    "**Model:** LightGBM regression (target = rank_loss)\n",
    "\n",
    "**Walk-forward for g(x):** Expanding window on residuals:\n",
    "- For fold k (k >= 20): train g(x) on all residuals from folds 1..k-1\n",
    "- Predict fold k's residuals -> g(x) predictions for ~89 folds\n",
    "- Start from fold 20 (~35K residual rows) for sufficient training data\n",
    "- Folds 1-19 get no g(x) prediction\n",
    "\n",
    "**g(x) features** (candidate set -- ablate to find the useful ones):\n",
    "\n",
    "| Feature | Source | Rationale |\n",
    "|---------|--------|-----------|\n",
    "| `score` (LGB prediction) | eval_rows | Extreme predictions may carry higher ranking error |\n",
    "| `abs_score` | derived | Magnitude -> confidence proxy |\n",
    "| `vol_20d` | regime_context | Per-stock noise level |\n",
    "| `vol_60d` | regime_context | Longer-term vol |\n",
    "| `mom_1m` | regime_context | Model's top feature; may predict own ranking failures |\n",
    "| `adv_20d` | eval_rows | Liquidity (low liquidity -> harder to rank) |\n",
    "| `vix_percentile_252d` | regime_context | Strongest epistemic predictor (rho = -0.21 with RankIC) |\n",
    "| `market_regime` | regime_context | Bull/neutral/bear (encoded) |\n",
    "| `market_vol_21d` | regime_context | Market-level noise |\n",
    "| `market_return_21d` | regime_context | Recent market direction |\n",
    "| `cross_sectional_rank` | derived | Stock's rank-normalized score within date |\n",
    "\n",
    "**Recommended density feature** (theoretically justified by Kotelevskii et al., NeurIPS 2022):\n",
    "- `knn_distance_5`: Euclidean distance to 5th-nearest training neighbor in PCA-reduced (~5 dims) feature space\n",
    "- The NUQ paper (Kotelevskii et al., 2022) shows epistemic uncertainty is proportional to sigma^2(x)/p(x) -- label variance divided by data density. In regions where p(x) is low (few nearby training examples), epistemic uncertainty is high regardless of model confidence.\n",
    "- This is the only feature that captures \"how unusual is this prediction context compared to training data?\" -- the other features capture market conditions and stock characteristics, not distributional novelty.\n",
    "- Implementation: PCA-reduce g(x) feature space to ~5 dims, use sklearn NearestNeighbors on training set, compute distance to 5th neighbor for each prediction. ~2 hours to implement.\n",
    "- Can be added as a g(x) feature improvement after initial 13.1 run and compared via ablation.\n",
    "\n",
    "**Hyperparameters:** Simpler than primary model (avoid overfitting the error predictor):\n",
    "- `n_estimators=50`, `max_depth=3`, `num_leaves=8`, `min_child_samples=50`\n",
    "\n",
    "**Secondary g(x):** Also train on MAE target with same features for robustness comparison.\n",
    "\n",
    "**Output:** g(x) predictions for each (date, ticker, horizon) from fold 20 onwards\n",
    "\n",
    "**Tests:**\n",
    "- g(x) is trained out-of-sample (no fold k residuals used to predict fold k)\n",
    "- g(x) > 0 for >99% of predictions (rank_loss is non-negative)\n",
    "- g(x) correlates with realized rank_loss (Spearman rho > 0.05 on held-out folds)\n",
    "\n",
    "---\n",
    "\n",
    "### 13.2 Aleatoric Baseline a(x)\n",
    "\n",
    "a(x) estimates irreducible ranking noise -- the floor of rank displacement that ANY model would experience at this point.\n",
    "\n",
    "**Key insight from data analysis:** vol_20d does NOT predict rank_loss (rho = 0.054). So the standard DEUP approach of a(x) = scaled_vol is WRONG for rank-based loss. Ranking noise comes from a different source than return volatility: it comes from how LITTLE cross-sectional differentiation exists on a given date. When all AI stocks move together (low dispersion), ranking is meaningless -- that's high aleatoric noise. When stocks differentiate (high dispersion), ranking is easy -- that's low aleatoric noise.\n",
    "\n",
    "**Critical direction rule: a(x) must be HIGH when ranking is HARD (low dispersion) and LOW when ranking is EASY (high dispersion).** All tier formulas below respect this inversion.\n",
    "\n",
    "**Important subtlety:** Raw cross-sectional dispersion (IQR) has a known weakness: it measures \"were outcomes spread out?\" not \"were outcomes PREDICTABLY spread out given available information?\" A day where stocks randomly scatter +/-20% has high IQR but is hard to rank (noise dispersion). A day where one stock dominates and everything else is flat also has high IQR but is trivially easy to rank (signal dispersion). Raw IQR conflates the two. We address this with a tiered approach.\n",
    "\n",
    "**Tier 0 -- Inverse IQR dispersion (baseline, implement first):**\n",
    "```\n",
    "a(date) = c / (IQR(excess_returns on date d) + eps)\n",
    "```\n",
    "where c is calibrated so median a(date) ~ median rank_loss, and eps prevents division by zero.\n",
    "- High dispersion (easy to rank) -> low a(date). Low dispersion (hard to rank) -> high a(date).\n",
    "- Fastest to implement (~30 min)\n",
    "- Shared across all stocks on a date (per-date, not per-stock)\n",
    "- Known weakness: doesn't condition on available information\n",
    "\n",
    "**Tier 1 -- Inverse factor-residual dispersion (upgrade if Tier 0 fails alignment diagnostic):**\n",
    "```\n",
    "For each date t:\n",
    "  1. Regress cross-sectional excess returns on sector dummies + market_return_21d + mom_1m\n",
    "     (simple factors any reasonable model could capture)\n",
    "  2. Take residuals = excess_return - predicted\n",
    "  3. a(date) = c / (IQR(residuals on date t) + eps)\n",
    "```\n",
    "- High residual dispersion = stocks differentiate BEYOND factors = easier to rank = low a(date)\n",
    "- Low residual dispersion = pure noise after factors = harder to rank = high a(date)\n",
    "- Measures \"unexplained cross-sectional mess\" after removing trivial structure\n",
    "- Still per-date, still fast\n",
    "- Paper-defensible: \"We approximate irreducible ranking noise as the inverse of cross-sectional dispersion of factor-residual returns\"\n",
    "- Add ~2-3 hours implementation time\n",
    "\n",
    "**Tier 2 -- Heteroscedastic per-stock noise (implement if you want the strongest result, ~3-4h):**\n",
    "```\n",
    "Train LGB quantile regression (Q25, Q75) of per-stock rank displacement\n",
    "on [vol_20d, adv_20d, sector, cross_sectional_dispersion]. Walk-forward fitted.\n",
    "a(stock, date) = predicted IQR = Q75(rank_loss | features) - Q25(rank_loss | features)\n",
    "```\n",
    "- Per-stock aleatoric noise: some stocks are inherently harder to rank (noisy small-caps vs stable large-caps)\n",
    "- Connects directly to the heteroscedastic likelihood literature (learning sigma^2(x) that varies with inputs)\n",
    "- Unlike heteroscedastic NNs that learn sigma^2 via negative log-likelihood, this uses tree-compatible quantile regression\n",
    "- Walk-forward fitting required for PIT safety\n",
    "- No inversion needed: directly predicts per-stock rank_loss IQR (high predicted IQR = hard to rank = high a)\n",
    "- This is the first tier that produces per-stock (not just per-date) aleatoric estimates\n",
    "\n",
    "**Tier 3 -- EXCLUDED (posterior-predictive simulation Bayes risk):**\n",
    "Intentionally not implemented. Uses the same features as Tier 2 (vol, adv, sector, VIX, mom). If Tier 2's quantile regression cannot predict 20d rank_loss IQR from those features (rho = 0.024), a simulation wrapper won't create signal that isn't there. The bottleneck is information content, not model complexity.\n",
    "\n",
    "**Prospective empirical (PIT-safe, deployment-ready):**\n",
    "```\n",
    "a(date) = P10(rank_loss over trailing 60 trading days, strictly before current date)\n",
    "```\n",
    "- Uses only historical information -- no same-date leakage\n",
    "- Works when ranking difficulty is persistent (autocorrelation > 0.5)\n",
    "- RESULTS: PASS at 90d (rho=0.516), MARGINAL at 20d (rho=0.192)\n",
    "- 20d autocorrelation at lag-20 is only 0.061 -- difficulty changes too fast\n",
    "\n",
    "**Same-date empirical (last resort, retrospective only):**\n",
    "```\n",
    "a(date) = P10(rank_loss on date d)\n",
    "```\n",
    "- High correlation but partially circular (uses same-date information)\n",
    "- Not deployment-ready: requires knowing today's rank_losses\n",
    "- Used only at 20d where no prospective approach works\n",
    "\n",
    "**RESULTS (actual tier selection):**\n",
    "- 20d: empirical same-date (rho=0.527) -- retrospective only, no deployable a(x) exists\n",
    "- 60d: Tier 2 (rho=0.317) -- per-stock, walk-forward, deployable\n",
    "- 90d: prospective P10 (rho=0.516) -- PIT-safe, deployable\n",
    "\n",
    "**Critical insight: a(x) precision matters less than expected for per-date tiers.**\n",
    "When a(x) is per-date, the per-stock RANKING of e_hat(x) = g(x) - a(date) is determined entirely by g(x). The real test is Diagnostic A/D in 13.4.\"\n",
    "\n",
    "**a(x) alignment diagnostic (MUST PASS before proceeding to 13.3):**\n",
    "After computing a(x) at any tier, run this validation:\n",
    "1. Bin dates into quintiles by a(date)\n",
    "2. For EACH quintile, compute median rank_loss for ALL models (LGB, Rank Avg 2, Learned Stacking)\n",
    "3. Expected: monotonically increasing rank_loss as a(date) increases\n",
    "4. Compute: rho(a(date), median rank_loss across dates)\n",
    "   - Target: rho > 0.3 (a captures meaningful variation in ranking difficulty)\n",
    "   - Kill: rho < 0.1 (a is not measuring ranking difficulty at all -> upgrade tier)\n",
    "5. Additional: compute g(date) - a(date) averaged per date, plot against realized daily RankIC\n",
    "   - e_hat should spike when MODEL fails more than baseline difficulty\n",
    "   - If e_hat spikes when a(date) is high AND rank_loss is high for ALL models, that's a(x) being too low (underestimating irreducible noise), not epistemic failure\n",
    "\n",
    "**Per-date vs per-stock note:**\n",
    "- a(x) is per-DATE, not per-stock (all ~100 stocks share the same a(date))\n",
    "- All per-stock variation in e_hat comes from g(x), which IS per-stock\n",
    "- This is conceptually correct: aleatoric ranking noise is a property of the market on that day; epistemic uncertainty is a property of the model's knowledge about that specific stock\n",
    "- Exception: Tier 2 produces per-stock sigma_i which feeds into per-date a(date) via simulation, so the per-date aggregation still happens but is informed by per-stock noise levels\n",
    "\n",
    "**Output:** a(x) predictions for all evaluation rows, alignment diagnostic report\n",
    "\n",
    "**Tests:**\n",
    "- a(x) > 0 for all rows\n",
    "- a(x) varies meaningfully across dates (not constant)\n",
    "- Alignment diagnostic passes (rho > 0.3, monotonic quintiles)\n",
    "- a(x) < g(x) for at least some non-trivial fraction of rows\n",
    "\n",
    "---\n",
    "\n",
    "### 13.3 Epistemic Signal e_hat(x) = max(0, g(x) - a(x))  \u2705 COMPLETE\n",
    "\n",
    "**Definition:** \u00ea(x) = max(0, g(x) \u2212 a(x))\n",
    "\n",
    "**Merge logic (tier-aware):**\n",
    "- 20d (per-date a): join on `as_of_date` \u2192 broadcast to all stocks \u2192 per-stock \u00ea ordering = g(x) ordering\n",
    "- 60d (per-stock Tier 2): join on `(as_of_date, ticker)` \u2192 per-stock a(x) creates selective zero/positive split\n",
    "- 90d (per-date prospective): join on `as_of_date` \u2192 broadcast to all stocks\n",
    "\n",
    "**Deployment labels:**\n",
    "- 20d: `retrospective_decomposition` (same-date a(x) not available at prediction time)\n",
    "- 60d/90d: `deployment_ready` (walk-forward / trailing data only)\n",
    "\n",
    "**Results:**\n",
    "| Horizon | \u00ea mean | % zero | % positive | \u03c1(\u00ea, rl) | Selective \u0394 |\n",
    "|---------|--------|--------|------------|----------|-------------|\n",
    "| 20d | 0.265 | 0.0% | 100% | 0.144 | +0.123 |\n",
    "| 60d | 0.004 | 85.2% | 14.8% | 0.106 | +0.012 |\n",
    "| 90d | 0.254 | 0.0% | 100% | 0.146 | +0.112 |\n",
    "\n",
    "**Critical finding \u2014 perfect per-stock quintile monotonicity (\u03c1 = 1.0) at 20d and 90d in both DEV and FINAL.** FINAL holdout shows stronger separation:\n",
    "- 20d: Q5/Q1 RL ratio 1.69 (FINAL) vs 1.51 (DEV)\n",
    "- 90d: Q5/Q1 RL ratio 1.88 (FINAL) vs 1.53 (DEV)\n",
    "\n",
    "**Holdout generalizes:** \u03c1(\u00ea, rl) is HIGHER in FINAL than DEV at all horizons (20d: 0.192 vs 0.142, 60d: 0.140 vs 0.103, 90d: 0.248 vs 0.138).\n",
    "\n",
    "**Daily negative \u03c1 at 20d explained:** \u00ea = g \u2212 a, and same-date a dominates date-level variation (\u03c1(\u00ea,a)=\u22120.815). On easy days (low a), \u00ea is high but rl is low \u2192 negative daily correlation. Per-stock ordering is unaffected.\n",
    "\n",
    "**Sanity checks: 14/14 passed.** Output: `ehat_predictions.parquet` (495,585 rows).\n",
    "\n",
    "---\n",
    "\n",
    "### 13.4 Diagnostics (DEV and FINAL separately) -- CRITICAL SECTION  \u2705 COMPLETE\n",
    "\n",
    "Six diagnostics + stability. Results for DEV and FINAL holdout separately.\n",
    "\n",
    "**Diagnostic A (Disentanglement): PASS at all horizons.** After residualizing on [vol_20d, vix, mom_1m], \u00ea still predicts rank_loss (\u03c1 = 0.11\u20130.24). \u00ea is NOT repackaged vol or VIX. FINAL is stronger than DEV.\n",
    "\n",
    "**Diagnostic B (Directional Confidence Stratification \u2014 not the target): N/A.** \u00ea predicts error MAGNITUDE (rank displacement), not directional accuracy (RankIC). High-\u00ea stocks actually have slightly higher RankIC because extreme-scored stocks are correct more often but have larger errors when wrong. The quintile monotonicity (13.3, \u03c1=1.0) is the correct test, not IC stratification. Renamed from \"FAIL\" to \"N/A\" because this tests the wrong property.\n",
    "\n",
    "**Diagnostic C (AUROC): 60d daily AUROC = 0.611 (PASS).** Stock-level AUROC for high-loss: 0.56\u20130.61. 20d daily AUROC = 0.33 (same-date a(x) artifact inverts date-level signal).\n",
    "\n",
    "**Diagnostic D (2024 Test): WEAK.** \u00ea is a per-stock error predictor, not a market-level regime detector. The 2024 collapse affects all stocks uniformly. Date-level throttling needs separate infrastructure.\n",
    "\n",
    "**Diagnostic E (Baselines): \u00ea/g(x) dominate by 3\u201310\u00d7.** \u03c1 with rank_loss: \u00ea = 0.14\u20130.19, g(x) = 0.16\u20130.22, vol_20d = 0.01\u20130.05, VIX = \u22120.02\u20130.05. Vol and VIX collapse in FINAL holdout; DEUP does not.\n",
    "\n",
    "**Diagnostic F (Features): Per-prediction features dominate (40\u201356%).** cross_sectional_rank is #1 at all horizons. VIX/regime contribute 12\u201319%. g(x) is NOT an expensive regime detector.\n",
    "\n",
    "**Stability: PASS.** All conditions positive (pre/post-2020, low/high VIX, low/high vol stocks). Min \u03c1 = 0.031, max \u03c1 = 0.205.\n",
    "\n",
    "**Full portfolio-level baseline comparison (8-way including sub-model disagreement, seed ensemble, trailing RankIC, conformal width) deferred to 13.6 where sizing formula is applied.**\n",
    "\n",
    "---\n",
    "\n",
    "### 13.4b Expert Health H(t) \u2014 Per-Date Regime Throttle  \u2705 COMPLETE\n",
    "\n",
    "**Motivation:** Diagnostic D shows \u00ea(x) is per-stock, not per-date. The 2024 collapse is uniform across stocks. Need a date-level throttle.\n",
    "\n",
    "**Two complementary signals:**\n",
    "- `\u00ea(x)` = cross-sectional position-level risk control (which names are dangerous today)\n",
    "- `H(t)` = expert-level regime control (whether the expert is usable today)\n",
    "\n",
    "**Three PIT-safe signals:**\n",
    "1. **H_realized:** Trailing EWMA of matured daily RankIC (lagged by horizon). Only component with genuine predictive power (\u03c1 = 0.065\u20130.210).\n",
    "2. **H_drift:** Feature drift + score drift + correlation spike vs trailing reference. Real-time (no lag). Mixed predictive value.\n",
    "3. **H_disagree:** Cross-expert ranking disagreement (LGB vs Rank Avg 2 vs Learned Stacking). Noisy but theoretically motivated.\n",
    "\n",
    "**Combination:** `H(t) = sigmoid(z_real \u2212 0.3\u00b7z_drift \u2212 0.3\u00b7z_disagree)`. G(t) = exposure multiplier \u2208 [0, 1].\n",
    "\n",
    "**Key Results:**\n",
    "- **20d Crisis Throttle (THE CENTRAL VALUE PROP):** H drops from 0.50 (Mar) \u2192 0.17 (Jun). G drops to 0.005 by May, 0.000 by June. System in full abstention mode for Apr\u2013Jul 2024. March losses not avoided (20d lag), but April\u2013July damage prevented.\n",
    "- **90d best overall:** \u03c1(H, RankIC) = 0.180, AUROC = 0.60 for predicting bad days.\n",
    "- **Regime separation works:** DEV mean_G = 0.37 vs FINAL mean_G = 0.15.\n",
    "- **Day-level prediction weak:** \u03c1 \u2248 0 within-period. H(t) is regime-level, not day-level.\n",
    "- **H_realized dominates.** Combined AUROC barely above realized-only.\n",
    "\n",
    "**Honest limitations:** (1) Cannot avoid first month of crisis (lag). (2) Drift/disagreement have limited marginal value. (3) 60d H is inverted due to lag.\n",
    "\n",
    "**Tests:** 18 passing (PIT safety, leakage alignment, date indexing, monotonicity). **Total Chapter 13: 116 tests.**\n",
    "\n",
    "**Files:** `src/uncertainty/expert_health.py`, `tests/test_expert_health.py`, orchestrator `--step 5`, outputs `expert_health_lgb_{20,60,90}d.parquet`.\n",
    "\n",
    "---\n",
    "\n",
    "### 13.5 Conformal Intervals (rolling, DEUP-normalized)  \u2705 COMPLETE\n",
    "\n",
    "Three nonconformity-score variants: raw, vol-normalized, DEUP-normalized. Rolling 60-day calibration, 90% nominal coverage.\n",
    "\n",
    "**Key Results:**\n",
    "- **All variants achieve ~90% marginal coverage** (ECE < 0.01). Conformal guarantees hold.\n",
    "- **DEUP conditional coverage spread: 0.8%** vs raw 20.2% vs vol 5.9% at 20d. **25\u00d7 improvement.** Raw conformal over-covers easy stocks (98%) and under-covers hard stocks (78%); DEUP equalizes to ~90% for both. This is the Plassier et al. validation.\n",
    "- **DEUP intervals are narrower** than raw (0.647 vs 0.675 at 20d) \u2014 more efficient, not wider.\n",
    "- **Width ratio:** DEUP 1.57\u00d7 wider for high-\u00ea vs low-\u00ea stocks \u2014 meaningful differentiation.\n",
    "- **60d special case:** 85% zero \u00ea \u2192 impractically wide intervals for zero-\u00ea stocks. DEUP conformal at 60d is meaningful only for the 15% with positive \u00ea.\n",
    "- **FINAL holdout:** DEUP maintains best coverage at 20d (0.898, ECE 0.002). 90d degrades slightly (0.87).\n",
    "\n",
    "**Cite:** \"DEUP-normalized conformal prediction approximates conditional validity by scaling nonconformity scores by predicted epistemic uncertainty, following the motivation of Plassier et al. (2025).\"\n",
    "\n",
    "**Tests:** 21 passing (PIT safety, coverage, widths, conditional coverage, sparse \u00ea, edge cases). **Total Chapter 13: 137 tests.**\n",
    "\n",
    "**Files:** `src/uncertainty/conformal_intervals.py`, `tests/test_conformal_intervals.py`, orchestrator `--step 6`, outputs `conformal_predictions.parquet`.\n",
    "\n",
    "---\n",
    "\n",
    "### 13.6 DEUP-Sized Shadow Portfolio + Global Regime Evaluation  \u2705 COMPLETE\n",
    "\n",
    "Economic test: does uncertainty-informed position sizing beat alternatives? Plus regime-level conclusion: does G(t) determine expert usability?\n",
    "\n",
    "**Sizing Variants (c calibrated on DEV only, median w \u2248 0.7):**\n",
    "- A) Vol-sized: `sized_score = score \u00d7 min(1, c_vol / sqrt(vol_20d + \u03b5))`\n",
    "- B) DEUP-sized: `sized_score = score \u00d7 min(1, c_deup / sqrt(unc + \u03b5))` (20d/90d: g(x), 60d: \u00ea(x))\n",
    "- C) Health-only: `return \u00d7 G(t)` (uniform date-level throttle)\n",
    "- D) Combined: `DEUP-sized return \u00d7 G(t)`\n",
    "\n",
    "**Key Portfolio Results (20d):**\n",
    "- **Vol-sized (A) wins:** FINAL Sharpe 1.68 vs 1.37 (raw) vs 1.35 (DEUP).\n",
    "- **DEUP per-stock sizing \u2248 raw:** g(x) penalizes extreme scores \u2014 the model's strongest signals.\n",
    "- **Health throttle:** Crisis MaxDD \u221217.5% vs \u221244.1% for raw (60% reduction).\n",
    "- **Optimal: G(t) as binary abstention gate, not continuous scaling.**\n",
    "\n",
    "**Regime-Trust Classifier (THE REGIME-LEVEL ANSWER):**\n",
    "- **H(t) AUROC = 0.721** (FINAL: 0.750). VIX: 0.449 (worse than random!).\n",
    "- **Bucket monotonicity \u03c1 = 1.0:** Low-G \u2192 \u22120.011 IC, 51% bad. High-G \u2192 +0.153 IC, 12% bad.\n",
    "- **Confusion matrix (G \u2265 0.2):** Precision 80%, Recall 64%, Abstention 47%.\n",
    "- **One-liner for Q&A:** DEUP does not improve per-name sizing, but it decisively improves whether to deploy the model at all via regime trust.\n",
    "- **Aggregated DEUP \u2260 regime signal** (AUROC \u2248 0.50). Regime trust from realized efficacy only.\n",
    "\n",
    "**Success criteria:** Regime trust AUROC > 0.65 (**PASS**: 0.72). Bucket monotonicity > 0.5 (**PASS**: 1.0). DEUP per-stock > vol (**FAIL** \u2014 honest finding).\n",
    "\n",
    "**Tests:** 17 passing. **Total Chapter 13: 154 tests.**\n",
    "\n",
    "**Files:** `src/uncertainty/deup_portfolio.py`, `tests/test_deup_portfolio.py`, orchestrator `--step 7`, outputs `chapter13_6_*.json/parquet`.\n",
    "---\n",
    "\n",
    "### 13.7 DEUP on Rank Avg 2 (optional but recommended)\n",
    "\n",
    "Rank Avg 2 held up better in holdout (20d FINAL RankIC: 0.031 vs 0.010).\n",
    "- Repeat 13.1-13.4 with Rank Avg 2 eval_rows (already populated in 13.0)\n",
    "- Compare e_hat properties: does a more robust base model produce different e_hat?\n",
    "- Compare portfolio: DEUP-sizing on Rank Avg 2 vs DEUP-sizing on LGB\n",
    "- Decision gate: if Rank Avg 2 + DEUP beats LGB + DEUP on FINAL, adopt as primary\n",
    "\n",
    "---\n",
    "\n",
    "### 13.8 Freeze & Documentation\n",
    "\n",
    "- Replace disagreement proxy in `AIStockForecasterExpert.epistemic_uncertainty()` with real DEUP e_hat\n",
    "- Implement `conformal_interval()` with rolling calibration\n",
    "- Save frozen e_hat predictions to `evaluation_outputs/chapter13/`\n",
    "- Save conformal interval calibration parameters\n",
    "- Save g(x) models and feature importances\n",
    "- Document all results in `documentation/CHAPTER_13.md`:\n",
    "  - Loss definition analysis (why rank displacement, not MAE)\n",
    "  - Bayes risk approximation: \"The decomposition e_hat = g(x) - a(x) requires a defensible estimate of irreducible ranking noise a(x). We evaluated four approximations of increasing sophistication: raw cross-sectional dispersion, factor-residual dispersion, heteroscedastic per-stock quantile regression, and posterior-predictive simulation. [Tier X] was selected based on an alignment diagnostic confirming that a(x) monotonically predicts rank_loss across all models, consistent with it capturing market-level ranking difficulty rather than model-specific failure.\"\n",
    "  - Variance decomposition framing (connect to literature): \"The standard predictive variance decomposition Var[y|x] = E[sigma^2(x)] + Var[mu(x)] separates aleatoric noise from epistemic uncertainty in return space. We adapt this decomposition to ranking space: g(x) estimates total expected rank displacement (analogous to total predictive variance), a(x) estimates irreducible ranking noise (analogous to E[sigma^2]), and e_hat(x) = g(x) - a(x) captures excess model failure (analogous to Var[mu]). Unlike heteroscedastic neural networks that learn sigma^2(x) via negative log-likelihood, our aleatoric estimate is derived from cross-sectional return dispersion -- a market-observable quantity that requires no model fitting -- making it robust to misspecification of the noise model.\" This connects to Robinson et al., Wong et al., and evidential regression literature while making clear why our approach differs (ranking loss, not return prediction; market-observable aleatoric, not learned).\n",
    "  - Full diagnostic tables (DEV + FINAL)\n",
    "  - 8-way UQ baseline comparison table\n",
    "  - Portfolio comparison (ALL + DEV + FINAL)\n",
    "  - The 2024 regime test\n",
    "  - g(x) feature importance analysis and interpretation\n",
    "  - Kill criteria assessment\n",
    "  - MC Dropout / deep ensemble exclusion justification\n",
    "  - Honest discussion of what worked and what didn't\n",
    "  - **Theoretical framing / citation block:**\n",
    "    - \"Our approach instantiates the pointwise risk decomposition of Kotelevskii & Panov (ICLR 2025), where Total Risk = Bayes Risk + Excess Risk. Since our base model (LightGBM) does not admit tractable Bayesian posterior inference, we follow the DEUP approach (Lahlou et al., 2023) to estimate excess risk directly from walk-forward residuals. The aleatoric component (Bayes risk) is approximated via cross-sectional return dispersion, which captures irreducible ranking difficulty -- the analog of -G(eta) in the proper scoring rule framework adapted to ranking loss.\"\n",
    "    - Cite Kotelevskii & Panov, \"From Risk to Uncertainty\" (ICLR 2025) -- theoretical framework for risk decomposition\n",
    "    - Cite Kotelevskii et al., \"Nonparametric UQ for Single Deterministic NN\" (NeurIPS 2022) -- density-based epistemic uncertainty, justifies knn_distance feature\n",
    "    - Cite Plassier et al., \"Probabilistic Conformal Prediction\" (ICLR 2025) -- conditional coverage motivation\n",
    "    - Cite Fishkov et al., \"UQ for Regression using Proper Scoring Rules\" (2025) -- extends framework to regression; our ranking problem sits between classification and regression\n",
    "    - Frame thesis explicitly: \"Our primary model is tree-based (LightGBM), which precludes standard Bayesian UQ methods (MC Dropout, variational inference). Our sub-models include neural networks (FinText/Chronos, FinBERT), but these produce near-zero standalone signal (Ch11), making their Bayesian uncertainty estimates uninformative for the primary ranking task. We therefore adopt the DEUP approach (Lahlou et al., 2023), which instantiates the pointwise risk decomposition of Kotelevskii & Panov (ICLR 2025) without requiring Bayesian posterior inference. We compare against sub-model disagreement -- an approximation of Expected Pairwise Bregman Divergence (EPBD) -- and LGB seed ensemble variance -- an approximation of Bregman Information -- as baselines (Diagnostic E). For Bayesian UQ via MC Dropout on the neural sub-models, see Chapter 17.\"\n",
    "  - **What NOT to implement** (and why): Full Bayesian posterior estimation, energy-based uncertainty, and the 9 risk approximation variants from Kotelevskii & Panov are designed for neural network ensembles with multiple forward passes. LightGBM doesn't support this. The seed ensemble baseline (#7 in Diagnostic E) captures the one applicable variant.\n",
    "- Update `ROADMAP.md` and `outline.ipynb`\n",
    "\n",
    "---\n",
    "\n",
    "### Success Criteria (adapted from UQ reference S12)\n",
    "\n",
    "| Metric | Target | What It Proves |\n",
    "|--------|--------|----------------|\n",
    "| rho(e_hat, vol \\| features) | ~ 0 | e_hat is NOT repackaged volatility |\n",
    "| rho(e_hat, rank_loss \\| features) | > 0 | e_hat captures real ranking-failure signal |\n",
    "| Low-e_hat tercile RankIC | > full-set | Model knows when to trust itself |\n",
    "| AUROC (e_hat predicts bad days) | > 0.60 | e_hat identifies regime failures |\n",
    "| e_hat spikes in Mar-Jul 2024 | Yes | DEUP detects holdout regime shift |\n",
    "| ECE (conformal) | < 0.05 | Intervals well-calibrated |\n",
    "| Coverage (90% nominal) | 85-95% | Rolling conformal maintains validity |\n",
    "| a(x) vs rank_loss correlation | rho > 0.3 | Aleatoric baseline captures real ranking difficulty |\n",
    "| e_hat-sized Sharpe | > ALL baselines | Economic value beyond any alternative |\n",
    "\n",
    "### Kill Criteria (from UQ reference S14.7)\n",
    "\n",
    "| Signal | Threshold | Action |\n",
    "|--------|-----------|--------|\n",
    "| rho(e_hat, vol \\| features) > 0.5 | e_hat is just volatility | DEUP failed |\n",
    "| Selective risk ~ full risk | e_hat has no power | DEUP failed |\n",
    "| Coverage < 70% or > 99% | Conformal broken | Fix calibration |\n",
    "| e_hat-sized <= vol AND <= trailing RankIC | No value vs simple methods | DEUP failed |\n",
    "\n",
    "Even if DEUP disappoints, the infrastructure (residual archive, conformal intervals, expert interface, baseline comparison framework) is useful regardless. The 8-way comparison table is publishable independently of DEUP's success.\n",
    "\n",
    "### Excluded methods (with thesis justification)\n",
    "- **MC Dropout:** inapplicable to tree-based models (LightGBM has no dropout layers)\n",
    "- **Deep ensembles (NN):** requires rebuilding the primary model as a neural network -- different model, not different UQ\n",
    "- **BNNs:** same -- requires model rebuild\n",
    "- **NGBoost:** different model family (natural gradient boosting outputs full distributions). Tests a different model, not a different UQ method on the same model. Potential Ch17 comparison if time allows.\n",
    "- **cSGLB (Amazon cyclical MCMC):** interesting but complex; no public LightGBM integration available\n",
    "- **Heteroscedastic likelihood / evidential regression:** requires neural network architectures with learned sigma^2(x) outputs. Our primary model is LightGBM (tree-based), which does not natively support heteroscedastic likelihood training. DEUP's advantage is model-agnosticism -- it produces uncertainty estimates for any base model given only held-out residuals. We compare against LGB quantile regression (Diagnostic E, baseline #6) as the closest tree-compatible probabilistic alternative. If a reviewer asks \"why not heteroscedastic likelihood?\", this is the answer.\n",
    "\n",
    "### Existing infrastructure to reuse\n",
    "- `src/models/residual_archive.py` -- ResidualArchive + AIStockForecasterExpert (Ch11.4)\n",
    "- `src/models/fusion_scorer.py` -- `compute_disagreement()` for sub-model uncertainty (Ch11.4)\n",
    "- `data/regime_context.parquet` -- 201K rows x 16 features (Ch12.4)\n",
    "- `scripts/run_chapter12_heuristics.py` -- shadow portfolio / portfolio metrics code\n",
    "- `src/evaluation/baselines.py` -- LightGBM training utilities\n",
    "- All eval_rows (LGB 591K, Rank Avg 2 844K, Vol-Sized 591K) from prior chapters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14) Monitoring & Research Ops\n",
    "\n",
    "- Prediction logging with timestamps\n",
    "- Matured-label scoring\n",
    "- Feature and performance drift detection\n",
    "\n",
    "Alerts:\n",
    "- RankIC decay\n",
    "- Calibration breakdown\n",
    "- Ranking instability\n",
    "\n",
    "---\n",
    "\n",
    "### 14.x Monitoring KPIs (Signal + Shadow Portfolio)\n",
    "\n",
    "Once the Shadow Portfolio is defined (Chapter 11), add monitoring hooks for professional KPIs:\n",
    "- RankIC drift (rolling)\n",
    "- Cost survival drift\n",
    "- Shadow portfolio Sharpe/IR drift\n",
    "- Drawdown alarms (rolling windows)\n",
    "- Turnover / cost drag spikes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15) Outputs & Interfaces\n",
    "\n",
    "- Ranked stock lists\n",
    "- Per-stock explanation summaries\n",
    "- Batch scoring interface\n",
    "- Full traceability of inputs and decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16) Global Research Acceptance Criteria\n",
    "\n",
    "A model is considered **valid** if:\n",
    "\n",
    "- Median walk-forward RankIC exceeds baseline by \u2265 0.02\n",
    "- Net-of-cost performance positive in \u2265 70% of folds\n",
    "- Top-10 ranking churn < 30% month-over-month\n",
    "- Performance degrades gracefully under regime shifts\n",
    "- No PIT or survivorship violations detected\n",
    "\n",
    "**Institutional-grade add-on (evaluation-only):**\n",
    "- Shadow portfolio sanity: Sharpe/IR is meaningfully > 0 and not driven by 1\u20132 months\n",
    "\n",
    "(Report this via the Chapter 11 Shadow Portfolio mapping; do not optimize to it.)\n",
    "\n",
    "---\n",
    "\n",
    "### 16.1 Risk Attribution / Factor Decomposition (acceptance gate)\n",
    "\n",
    "**Purpose:** Prove the signal isn't just repackaged factor exposure.\n",
    "\n",
    "Chapter 12 answers *\"when does the model fail?\"* (regime slicing).\n",
    "This section answers *\"where does the alpha come from?\"* (return attribution).\n",
    "These are complementary but distinct \u2014 regime analysis decomposes performance\n",
    "by market conditions; risk attribution decomposes returns by factor exposures.\n",
    "\n",
    "**Existing infrastructure:**\n",
    "- `src/features/neutralization.py` (Ch5): cross-sectional *feature*-level neutralization\n",
    "  (\"Is this score just a sector/beta proxy?\")\n",
    "- This section adds *portfolio*-level return attribution\n",
    "  (\"Are our portfolio returns just market/size/value/momentum exposures?\")\n",
    "\n",
    "**Implementation:**\n",
    "1. Load Fama-French 5-factor daily returns (Ken French data library or `pandas-datareader`)\n",
    "   - Factors: Mkt-RF, SMB, HML, RMW, CMA (+ RF for excess returns)\n",
    "2. Align to shadow portfolio return dates (non-overlapping monthly from Ch11/12)\n",
    "3. OLS regression: `R_portfolio - RF = \u03b1 + \u03b2\u2081\u00b7MktRF + \u03b2\u2082\u00b7SMB + \u03b2\u2083\u00b7HML + \u03b2\u2084\u00b7RMW + \u03b2\u2085\u00b7CMA + \u03b5`\n",
    "4. Report: alpha intercept, factor betas, t-statistics, R\u00b2\n",
    "\n",
    "**Acceptance criteria:**\n",
    "- Alpha intercept is **positive and statistically significant** (t-stat > 2)\n",
    "- Factor loadings documented (some momentum exposure is acceptable, but alpha must survive)\n",
    "- R\u00b2 documented (low R\u00b2 = genuinely idiosyncratic alpha)\n",
    "\n",
    "**Scope:** ~1 day implementation. Run on LGB baseline + vol-sized shadow portfolio returns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17) Bayesian UQ Extensions & Model Comparisons\n",
    "\n",
    "**Goal:** Extend the DEUP-based UQ framework with Bayesian uncertainty estimation on the neural sub-models (FinText, FinBERT) and compare against the tree-based DEUP approach from Chapter 13. This chapter is the \"what if we could do Bayesian inference?\" comparison.\n",
    "\n",
    "**Prerequisite:** Chapter 13 DEUP must be complete with all diagnostics.\n",
    "\n",
    "**Context:** The Kotelevskii & Panov (ICLR 2025) risk decomposition framework offers 9 approximation variants for Bayesian risk estimation, most of which require multiple forward passes through a neural network. Chapter 13 uses DEUP because the primary model (LightGBM) doesn't support Bayesian inference. This chapter tests whether Bayesian approaches on the available NNs add value.\n",
    "\n",
    "---\n",
    "\n",
    "### 17.1 MC Dropout on FinBERT Sentiment\n",
    "\n",
    "- Run FinBERT inference 10-20 times per stock per date with dropout enabled\n",
    "- Compute variance of sentiment scores across passes = epistemic uncertainty of the sentiment model\n",
    "- Evaluate: does FinBERT dropout variance predict LGB ranking failures?\n",
    "- Compare: FinBERT dropout variance vs DEUP e_hat as sizing signal\n",
    "- Cost: significant (20x inference per fold x 109 folds), but FinBERT is small enough\n",
    "\n",
    "### 17.2 MC Dropout on FinText/Chronos\n",
    "\n",
    "- Run Chronos inference with dropout enabled (multiple forward passes)\n",
    "- Compute distribution of return forecasts per stock per date\n",
    "- Variance across passes = epistemic uncertainty of the time series model\n",
    "- Cost: expensive (Chronos is 46M params), may need to subsample folds\n",
    "\n",
    "### 17.3 Bayesian Risk Estimates (Kotelevskii & Panov Framework)\n",
    "\n",
    "Use the MC Dropout samples to compute the formal Bayesian risk decomposition:\n",
    "- R_Bayes (aleatoric) via ensemble averaging of per-pass predictions\n",
    "- R_Exc (epistemic) via Bregman divergence between ensemble mean and individual passes\n",
    "- Compare against DEUP's simpler g(x) - a(x) decomposition\n",
    "- Frame: \"When Bayesian inference IS tractable (NN sub-models), does it outperform DEUP?\"\n",
    "\n",
    "### 17.4 Seed Ensemble as Bregman Information\n",
    "\n",
    "- The 5-LGB-seed ensemble from Ch13 Diagnostic E baseline #7 already enables this\n",
    "- Frame explicitly as R_Exc^(1,2) = Bregman Information = variance of predictions across seed models (Kotelevskii & Panov, 2025)\n",
    "- Compare: seed ensemble Bregman Information vs DEUP e_hat vs MC Dropout variance\n",
    "\n",
    "### 17.5 NGBoost Comparison (optional)\n",
    "\n",
    "- Natural Gradient Boosting outputs full probability distributions (not just point estimates)\n",
    "- Different model family, not just different UQ method on same model\n",
    "- Tests whether a natively probabilistic tree model outperforms DEUP on a deterministic tree model\n",
    "- ~4 hours implementation\n",
    "\n",
    "### 17.6 Comparison Table & Documentation\n",
    "\n",
    "Produce the definitive UQ comparison table:\n",
    "\n",
    "| Method | Model | Type | Sharpe | Max DD | AUROC |\n",
    "|--------|-------|------|--------|--------|-------|\n",
    "| DEUP e_hat (Ch13) | LGB | Excess risk | ? | ? | ? |\n",
    "| Vol-sizing (Ch12) | Any | Heuristic | 2.73 | -18.1% | - |\n",
    "| Sub-model disagreement (EPBD) | LGB+FinText+Sent | Ensemble | ? | ? | ? |\n",
    "| Seed ensemble (Bregman Info) | LGB x5 | Ensemble | ? | ? | ? |\n",
    "| MC Dropout FinBERT | FinBERT | Bayesian | ? | ? | ? |\n",
    "| MC Dropout Chronos | Chronos | Bayesian | ? | ? | ? |\n",
    "| NGBoost | NGBoost | Probabilistic | ? | ? | ? |\n",
    "\n",
    "**Thesis framing:** \"We compare three paradigms of uncertainty quantification: (1) DEUP-based excess risk estimation for tree models, (2) ensemble disagreement as EPBD/Bregman Information approximations, and (3) Bayesian posterior sampling via MC Dropout on neural sub-models. This provides the first comprehensive comparison of modern UQ methods on a real financial ranking system with point-in-time safe evaluation.\"\n",
    "\n",
    "**Note:** The primary finding may be that DEUP on a strong base model (LGB) outperforms Bayesian UQ on weak sub-models (FinText, Sentiment), which is itself an interesting result -- model strength matters more than UQ sophistication.\n",
    "\n",
    "### Estimated effort\n",
    "- 17.1 MC Dropout FinBERT: 4-6 hours (inference + evaluation)\n",
    "- 17.2 MC Dropout Chronos: 6-8 hours (expensive inference)\n",
    "- 17.3 Bayesian risk estimates: 2-3 hours (computation on existing samples)\n",
    "- 17.4 Seed ensemble framing: 1 hour (already computed in Ch13)\n",
    "- 17.5 NGBoost: 4 hours (new model family)\n",
    "- 17.6 Documentation: 2-3 hours\n",
    "- **Total: ~20-25 hours**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}